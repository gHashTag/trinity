# ═══════════════════════════════════════════════════════════════════════════════
# TRINITY INFERENCE - Production Deployment
# ═══════════════════════════════════════════════════════════════════════════════
# Auto-scaling LLM inference with health checks and monitoring
# Sacred Formula: V = n × 3^k × π^m × φ^p × e^q
# Golden Identity: φ² + 1/φ² = 3
# ═══════════════════════════════════════════════════════════════════════════════

app = "trinity-inference"
primary_region = "iad"

[build]
  dockerfile = "Dockerfile.inference"

[env]
  # Inference configuration
  NUM_THREADS = "16"
  MAX_BATCH_SIZE = "32"
  MAX_SEQUENCE_LENGTH = "4096"
  
  # Monitoring
  METRICS_PORT = "9090"
  HEALTH_PORT = "8081"
  
  # Scaling thresholds
  TARGET_CPU_PERCENT = "70"
  TARGET_QUEUE_DEPTH = "50"
  TARGET_TTFT_MS = "100"

# Main inference service
[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = false
  auto_start_machines = true
  min_machines_running = 1
  processes = ["app"]

  [http_service.concurrency]
    type = "requests"
    hard_limit = 100
    soft_limit = 80

  [http_service.machine_checks]
    grace_period = "30s"
    interval = "15s"
    timeout = "5s"
    path = "/health/ready"

# Metrics service (Prometheus)
[[services]]
  internal_port = 9090
  protocol = "tcp"
  
  [[services.ports]]
    port = 9090

# Health check service
[[services]]
  internal_port = 8081
  protocol = "tcp"
  
  [[services.ports]]
    port = 8081

# VM configuration - performance tier for inference
[[vm]]
  cpu_kind = "performance"
  cpus = 4
  memory_mb = 8192

# Health checks
[checks]
  [checks.liveness]
    grace_period = "10s"
    interval = "10s"
    method = "GET"
    path = "/health/live"
    port = 8081
    timeout = "3s"
    type = "http"

  [checks.readiness]
    grace_period = "30s"
    interval = "15s"
    method = "GET"
    path = "/health/ready"
    port = 8081
    timeout = "5s"
    type = "http"

  [checks.startup]
    grace_period = "60s"
    interval = "5s"
    method = "GET"
    path = "/health/startup"
    port = 8081
    timeout = "10s"
    type = "http"

# Scaling configuration
[experimental]
  auto_rollback = true

# Mounts for model storage
[[mounts]]
  source = "trinity_models"
  destination = "/app/models"
  initial_size = "10gb"

# Secrets (set via fly secrets set)
# FLY_API_TOKEN - for scaling API
# MODEL_PATH - path to model file
