# ═══════════════════════════════════════════════════════════════════════════════
# TRINITY PARALLEL DEQUANTIZATION (OPT-003)
# Multi-threaded weight loading for faster model startup
# φ² + 1/φ² = 3 = TRINITY
# ═══════════════════════════════════════════════════════════════════════════════

name: parallel_dequantization
version: "1.0.0"
language: zig
module: parallel_dequantization

# ═══════════════════════════════════════════════════════════════════════════════
# PROBLEM ANALYSIS
# ═══════════════════════════════════════════════════════════════════════════════

# Current state:
# - Model load time: 208 seconds for SmolLM2-1.7B
# - Dequantization is sequential (single-threaded)
# - 16 CPU cores available but only 1 used
# - Bottleneck: Q8_0 dequantization loop

# Target:
# - Reduce load time to ~30-40 seconds (5-6x speedup)
# - Use all 16 cores for parallel dequantization
# - Maintain correctness (same output as sequential)

# ═══════════════════════════════════════════════════════════════════════════════
# TYPES
# ═══════════════════════════════════════════════════════════════════════════════

types:
  DequantizeTask:
    fields:
      tensor_name: String
      data: List<Int>         # Raw quantized bytes
      tensor_type: String     # Q8_0, Q4_0, Q4_K, etc.
      num_elements: Int
      output_offset: Int      # Where to write in output buffer

  DequantizeResult:
    fields:
      tensor_name: String
      time_ms: Float
      elements_processed: Int
      success: Bool

  ParallelConfig:
    fields:
      num_threads: Int
      chunk_size: Int         # Blocks per thread
      use_simd: Bool

  LoadMetrics:
    fields:
      total_time_ms: Float
      dequant_time_ms: Float
      io_time_ms: Float
      tensors_loaded: Int
      total_elements: Int

# ═══════════════════════════════════════════════════════════════════════════════
# PARALLELIZATION STRATEGY
# ═══════════════════════════════════════════════════════════════════════════════

# Strategy 1: Tensor-level parallelism
# - Each thread processes different tensors
# - Good for many small tensors
# - Simple implementation

# Strategy 2: Block-level parallelism (CHOSEN)
# - Split large tensor into chunks
# - Each thread dequantizes a chunk
# - Better for few large tensors (like weight matrices)

# Strategy 3: Hybrid
# - Use tensor-level for small tensors
# - Use block-level for large tensors (>1M elements)

parallelization_config:
  default_threads: 16
  min_elements_for_parallel: 100000  # 100K elements threshold
  chunk_size_blocks: 1024            # Blocks per chunk

# ═══════════════════════════════════════════════════════════════════════════════
# Q8_0 PARALLEL DEQUANTIZATION
# ═══════════════════════════════════════════════════════════════════════════════

# Q8_0 format:
# - Block size: 32 elements
# - Type size: 34 bytes (2 byte scale + 32 byte data)
# - Each block is independent (can parallelize)

q8_0_parallel:
  block_size: 32
  type_size: 34
  parallel_approach: |
    1. Calculate total blocks: num_blocks = (num_elements + 31) / 32
    2. Divide blocks among threads: blocks_per_thread = num_blocks / num_threads
    3. Each thread processes its block range independently
    4. No synchronization needed (blocks are independent)
    5. Use SIMD within each thread for scale multiplication

# ═══════════════════════════════════════════════════════════════════════════════
# BEHAVIORS
# ═══════════════════════════════════════════════════════════════════════════════

behaviors:
  - name: parallel_dequantize_q8_0
    given: Quantized data, num_elements, num_threads
    when: Large tensor dequantization requested
    then: Return f32 array using parallel processing

  - name: dequantize_chunk_q8_0
    given: Data slice, start_block, end_block, output slice
    when: Thread worker processes chunk
    then: Dequantize blocks in range to output

  - name: calculate_optimal_threads
    given: Num elements, available cores
    when: Thread count decision needed
    then: Return optimal thread count (min overhead)

  - name: parallel_load_weights
    given: GGUF file, model config
    when: Model loading requested
    then: Load all weights using parallel dequantization

  - name: benchmark_dequantization
    given: Tensor size, num_threads
    when: Performance measurement requested
    then: Return LoadMetrics with timing breakdown
