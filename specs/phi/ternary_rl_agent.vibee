name: ternary_rl_agent
version: "1.0.0"
language: zig
module: ternary_rl_agent

description: |
  Reinforcement Learning Agent using Ternary Hyperdimensional Computing.
  States and actions represented as ternary hypervectors.
  Online TD-learning with ternary quantization.
  
  Scientific Basis:
  - HDC for RL: Symbolic state/action representations
  - TD-Learning: Sutton & Barto temporal difference
  - Ternary Efficiency: BitNet-style weight compression

constants:
  STATE_DIM: 10240
  ACTION_DIM: 10240
  GAMMA: 0.99
  LEARNING_RATE: 0.01
  EPSILON_START: 1.0
  EPSILON_END: 0.01
  EPSILON_DECAY: 0.995

types:
  State:
    description: "Environment state as hypervector"
    fields:
      vector: HyperVector
      features: List<Float>

  Action:
    description: "Action representation"
    fields:
      id: Int
      vector: HyperVector
      name: String

  Experience:
    description: "Single transition tuple"
    fields:
      state: State
      action: Action
      reward: Float
      next_state: State
      done: Bool

  ValueFunction:
    description: "State value function as hypervector"
    fields:
      accumulator: FloatVector
      vector: HyperVector
      update_count: Int

  QFunction:
    description: "State-action value function"
    fields:
      state_action_values: Map<String, ValueFunction>
      action_seeds: List<HyperVector>

  Policy:
    description: "Action selection policy"
    fields:
      q_function: QFunction
      epsilon: Float
      action_space: List<Action>

  AgentConfig:
    description: "RL agent configuration"
    fields:
      state_dim: Int
      num_actions: Int
      gamma: Float
      learning_rate: Float
      epsilon_start: Float
      epsilon_end: Float
      epsilon_decay: Float

  RLAgent:
    description: "Complete RL agent"
    fields:
      config: AgentConfig
      policy: Policy
      value_function: ValueFunction
      episode_count: Int
      total_steps: Int
      total_reward: Float

  TrainingMetrics:
    description: "Training statistics"
    fields:
      episode_rewards: List<Float>
      avg_reward_100: Float
      epsilon_current: Float
      value_updates: Int

behaviors:
  # Initialization
  - name: create_agent
    given: AgentConfig
    when: Initializing new RL agent
    then: Returns RLAgent with random action seeds

  - name: create_action_seeds
    given: Number of actions and dimension
    when: Generating orthogonal action representations
    then: Returns list of random hypervectors

  # State Encoding
  - name: encode_state
    given: Raw state features
    when: Converting to hypervector
    then: Returns State with encoded vector

  - name: encode_continuous_state
    given: Continuous feature vector
    when: Discretizing and encoding
    then: Returns State using level hypervectors

  - name: encode_discrete_state
    given: Discrete state index
    when: Looking up state vector
    then: Returns pre-computed State hypervector

  # Action Selection
  - name: select_action
    given: State and Policy
    when: Choosing action (epsilon-greedy)
    then: Returns Action based on Q-values or random

  - name: select_action_greedy
    given: State and Policy
    when: Choosing best action
    then: Returns Action with highest Q-value

  - name: compute_q_value
    given: State, Action, and QFunction
    when: Estimating state-action value
    then: Returns float Q(s,a)

  # Learning
  - name: td_update
    given: Experience and Agent
    when: Performing TD(0) update
    then: Updates value function, returns TD error

  - name: sarsa_update
    given: Experience, next_action, and Agent
    when: Performing SARSA update
    then: Updates Q-function on-policy

  - name: q_learning_update
    given: Experience and Agent
    when: Performing Q-learning update
    then: Updates Q-function off-policy

  - name: batch_update
    given: List of Experiences and Agent
    when: Processing experience batch
    then: Updates all values, returns avg TD error

  # Quantization
  - name: quantize_value_function
    given: ValueFunction
    when: Converting float accumulator to ternary
    then: Returns quantized ValueFunction

  - name: online_value_update
    given: State, target_value, and ValueFunction
    when: Incrementally updating value estimate
    then: Updates accumulator and quantizes

  # Episode Management
  - name: start_episode
    given: Agent
    when: Beginning new episode
    then: Resets episode state, returns initial metrics

  - name: step
    given: Agent, State, Action, reward, next_State, done
    when: Processing single environment step
    then: Updates agent, returns TrainingMetrics

  - name: end_episode
    given: Agent
    when: Episode terminates
    then: Updates epsilon, logs metrics

  # Utility
  - name: decay_epsilon
    given: Agent
    when: Reducing exploration rate
    then: Updates epsilon with decay factor

  - name: get_metrics
    given: Agent
    when: Querying training statistics
    then: Returns TrainingMetrics

  - name: save_agent
    given: Agent and path
    when: Serializing trained agent
    then: Writes agent state to file

  - name: load_agent
    given: Path
    when: Loading pre-trained agent
    then: Returns Agent with loaded weights

tests:
  - name: test_action_seeds_orthogonal
    input:
      num_actions: 10
      dim: 10000
    expected:
      pairwise_similarity: < 0.1

  - name: test_q_value_computation
    input:
      state: random_state
      action: action_0
      q_function: initialized
    expected:
      q_value_in_range: [-100, 100]

  - name: test_td_update_reduces_error
    input:
      experience: (s, a, r=1.0, s', done=false)
      initial_value: 0.0
    expected:
      value_increases: true

  - name: test_epsilon_decay
    input:
      initial_epsilon: 1.0
      decay: 0.995
      steps: 100
    expected:
      final_epsilon: approx 0.606

  - name: test_greedy_selects_max
    input:
      q_values: [0.1, 0.5, 0.3]
    expected:
      selected_action: 1

  - name: test_quantization_preserves_sign
    input:
      float_values: [0.8, -0.6, 0.1]
    expected:
      ternary: [1, -1, 0]

  - name: test_simple_environment
    input:
      environment: gridworld_4x4
      episodes: 1000
    expected:
      avg_reward_improves: true
