// HDC Double Q-Learning for CartPole-v1
// Ï†Â² + 1/Ï†Â² = 3 | TRINITY
// Continuous state space with HDC encoding

const std = @import("std");
const print = std.debug.print;
const math = std.math;

// ============================================================================
// CONSTANTS
// ============================================================================

pub const DIMENSION: usize = 4096;
pub const N_ACTIONS: usize = 2; // left, right
pub const N_LEVELS: usize = 21; // Discretization levels for continuous values

// CartPole state bounds
const CART_POS_MIN: f32 = -2.4;
const CART_POS_MAX: f32 = 2.4;
const CART_VEL_MIN: f32 = -3.0;
const CART_VEL_MAX: f32 = 3.0;
const POLE_ANGLE_MIN: f32 = -0.21; // ~12 degrees
const POLE_ANGLE_MAX: f32 = 0.21;
const POLE_VEL_MIN: f32 = -3.0;
const POLE_VEL_MAX: f32 = 3.0;

// ============================================================================
// HYPERVECTOR
// ============================================================================

pub const Hypervector = struct {
    data: [DIMENSION]f32,

    pub fn init() Hypervector {
        return Hypervector{ .data = [_]f32{0.0} ** DIMENSION };
    }

    pub fn randomBipolar(rng: *std.Random) Hypervector {
        var hv = Hypervector.init();
        for (&hv.data) |*d| {
            d.* = if (rng.float(f32) < 0.5) -1.0 else 1.0;
        }
        return hv;
    }

    pub fn dot(self: Hypervector, other: Hypervector) f32 {
        var sum: f32 = 0.0;
        for (0..DIMENSION) |i| {
            sum += self.data[i] * other.data[i];
        }
        return sum;
    }

    pub fn addScaled(self: *Hypervector, other: Hypervector, scale: f32) void {
        for (0..DIMENSION) |i| {
            self.data[i] += scale * other.data[i];
        }
    }

    /// Permute (circular shift) for position encoding
    pub fn permute(self: Hypervector, shift: usize) Hypervector {
        var result = Hypervector.init();
        for (0..DIMENSION) |i| {
            result.data[i] = self.data[(i + shift) % DIMENSION];
        }
        return result;
    }

    /// Element-wise multiply (bind)
    pub fn bind(self: Hypervector, other: Hypervector) Hypervector {
        var result = Hypervector.init();
        for (0..DIMENSION) |i| {
            result.data[i] = self.data[i] * other.data[i];
        }
        return result;
    }

    /// Bundle (element-wise sum)
    pub fn bundle(vectors: []const Hypervector) Hypervector {
        var result = Hypervector.init();
        for (vectors) |v| {
            for (0..DIMENSION) |i| {
                result.data[i] += v.data[i];
            }
        }
        return result;
    }

    pub fn addNoise(self: *Hypervector, flip_rate: f32, rng: *std.Random) void {
        for (&self.data) |*d| {
            if (rng.float(f32) < flip_rate) {
                d.* = -d.*;
            }
        }
    }
};

// ============================================================================
// CONTINUOUS STATE ENCODER
// ============================================================================

pub const ContinuousStateEncoder = struct {
    // Level hypervectors for each feature
    level_hvs: [4][N_LEVELS]Hypervector, // 4 features, N_LEVELS each
    // Feature ID vectors
    feature_ids: [4]Hypervector,

    pub fn init(rng: *std.Random) ContinuousStateEncoder {
        var encoder = ContinuousStateEncoder{
            .level_hvs = undefined,
            .feature_ids = undefined,
        };

        // Generate feature ID vectors
        for (0..4) |f| {
            encoder.feature_ids[f] = Hypervector.randomBipolar(rng);
        }

        // Generate level hypervectors using similarity-preserving encoding
        // Adjacent levels should be similar
        for (0..4) |f| {
            // Start with random base
            const base = Hypervector.randomBipolar(rng);
            encoder.level_hvs[f][0] = base;

            // Each subsequent level flips ~5% of bits
            for (1..N_LEVELS) |l| {
                var new_hv = encoder.level_hvs[f][l - 1];
                const flip_count = DIMENSION / 20; // 5%
                for (0..flip_count) |_| {
                    const idx = rng.intRangeAtMost(usize, 0, DIMENSION - 1);
                    new_hv.data[idx] = -new_hv.data[idx];
                }
                encoder.level_hvs[f][l] = new_hv;
            }
        }

        return encoder;
    }

    /// Discretize continuous value to level index
    fn discretize(value: f32, min_val: f32, max_val: f32) usize {
        const clamped = @max(min_val, @min(max_val, value));
        const normalized = (clamped - min_val) / (max_val - min_val);
        const level: usize = @intFromFloat(normalized * @as(f32, N_LEVELS - 1));
        return @min(level, N_LEVELS - 1);
    }

    /// Encode continuous state to hypervector
    pub fn encode(self: ContinuousStateEncoder, state: [4]f32) Hypervector {
        // Discretize each feature
        const levels = [4]usize{
            discretize(state[0], CART_POS_MIN, CART_POS_MAX),
            discretize(state[1], CART_VEL_MIN, CART_VEL_MAX),
            discretize(state[2], POLE_ANGLE_MIN, POLE_ANGLE_MAX),
            discretize(state[3], POLE_VEL_MIN, POLE_VEL_MAX),
        };

        // Bind feature ID with level HV, then bundle all
        var bound_hvs: [4]Hypervector = undefined;
        for (0..4) |f| {
            bound_hvs[f] = self.feature_ids[f].bind(self.level_hvs[f][levels[f]]);
        }

        return Hypervector.bundle(&bound_hvs);
    }
};

// ============================================================================
// HDC Q-ESTIMATOR
// ============================================================================

pub const HDCQEstimator = struct {
    weights: [N_ACTIONS]Hypervector,

    pub fn init() HDCQEstimator {
        var estimator = HDCQEstimator{ .weights = undefined };
        for (0..N_ACTIONS) |i| {
            estimator.weights[i] = Hypervector.init();
        }
        return estimator;
    }

    pub fn computeQ(self: HDCQEstimator, state_vec: Hypervector, action: u8) f32 {
        return self.weights[action].dot(state_vec) / @as(f32, DIMENSION);
    }

    pub fn update(self: *HDCQEstimator, action: u8, state_vec: Hypervector, td_error: f32, lr: f32) void {
        self.weights[action].addScaled(state_vec, lr * td_error);
    }
};

// ============================================================================
// HDC DOUBLE Q AGENT FOR CARTPOLE
// ============================================================================

pub const HDCCartPoleAgent = struct {
    q1: HDCQEstimator,
    q2: HDCQEstimator,
    state_encoder: ContinuousStateEncoder,
    epsilon: f32,
    epsilon_min: f32,
    epsilon_decay: f32,
    learning_rate: f32,
    gamma: f32,
    rng: std.Random,

    pub fn init(seed: u64) HDCCartPoleAgent {
        var prng = std.Random.DefaultPrng.init(seed);
        var rng = prng.random();

        return HDCCartPoleAgent{
            .q1 = HDCQEstimator.init(),
            .q2 = HDCQEstimator.init(),
            .state_encoder = ContinuousStateEncoder.init(&rng),
            .epsilon = 1.0,
            .epsilon_min = 0.01,
            .epsilon_decay = 0.99,
            .learning_rate = 0.5, // Higher LR
            .gamma = 0.95,
            .rng = rng,
        };
    }

    pub fn chooseAction(self: *HDCCartPoleAgent, state: [4]f32) u8 {
        if (self.rng.float(f32) < self.epsilon) {
            return @intCast(self.rng.intRangeAtMost(u8, 0, N_ACTIONS - 1));
        }

        const state_vec = self.state_encoder.encode(state);
        const q0 = self.q1.computeQ(state_vec, 0) + self.q2.computeQ(state_vec, 0);
        const q1 = self.q1.computeQ(state_vec, 1) + self.q2.computeQ(state_vec, 1);

        return if (q1 > q0) 1 else 0;
    }

    pub fn tdUpdate(self: *HDCCartPoleAgent, state: [4]f32, action: u8, reward: f32, next_state: [4]f32, done: bool) void {
        const state_vec = self.state_encoder.encode(state);
        const next_state_vec = self.state_encoder.encode(next_state);

        if (self.rng.float(f32) < 0.5) {
            // Update Q1
            const q1_0 = self.q1.computeQ(next_state_vec, 0);
            const q1_1 = self.q1.computeQ(next_state_vec, 1);
            const a_star: u8 = if (q1_1 > q1_0) 1 else 0;

            const target = if (done) reward else reward + self.gamma * self.q2.computeQ(next_state_vec, a_star);
            const current = self.q1.computeQ(state_vec, action);
            const td_error = target - current;
            self.q1.update(action, state_vec, td_error, self.learning_rate);
        } else {
            // Update Q2
            const q2_0 = self.q2.computeQ(next_state_vec, 0);
            const q2_1 = self.q2.computeQ(next_state_vec, 1);
            const a_star: u8 = if (q2_1 > q2_0) 1 else 0;

            const target = if (done) reward else reward + self.gamma * self.q1.computeQ(next_state_vec, a_star);
            const current = self.q2.computeQ(state_vec, action);
            const td_error = target - current;
            self.q2.update(action, state_vec, td_error, self.learning_rate);
        }
    }

    pub fn decayEpsilon(self: *HDCCartPoleAgent) void {
        self.epsilon = @max(self.epsilon_min, self.epsilon * self.epsilon_decay);
    }
};

// ============================================================================
// CARTPOLE ENVIRONMENT
// ============================================================================

pub const CartPoleEnv = struct {
    // State: [cart_pos, cart_vel, pole_angle, pole_vel]
    state: [4]f32 = [_]f32{ 0.0, 0.0, 0.0, 0.0 },
    rng: std.Random,

    // Physics constants
    const gravity: f32 = 9.8;
    const cart_mass: f32 = 1.0;
    const pole_mass: f32 = 0.1;
    const total_mass: f32 = cart_mass + pole_mass;
    const pole_length: f32 = 0.5;
    const pole_mass_length: f32 = pole_mass * pole_length;
    const force_mag: f32 = 10.0;
    const tau: f32 = 0.02; // Time step

    pub fn init(seed: u64) CartPoleEnv {
        var prng = std.Random.DefaultPrng.init(seed);
        return CartPoleEnv{ .rng = prng.random() };
    }

    pub fn reset(self: *CartPoleEnv) [4]f32 {
        // Random initial state in [-0.05, 0.05]
        for (&self.state) |*s| {
            s.* = (self.rng.float(f32) - 0.5) * 0.1;
        }
        return self.state;
    }

    pub fn step(self: *CartPoleEnv, action: u8) struct { state: [4]f32, reward: f32, done: bool } {
        const x = self.state[0];
        const x_dot = self.state[1];
        const theta = self.state[2];
        const theta_dot = self.state[3];

        const force: f32 = if (action == 1) force_mag else -force_mag;

        const cos_theta = @cos(theta);
        const sin_theta = @sin(theta);

        const temp = (force + pole_mass_length * theta_dot * theta_dot * sin_theta) / total_mass;
        const theta_acc = (gravity * sin_theta - cos_theta * temp) /
            (pole_length * (4.0 / 3.0 - pole_mass * cos_theta * cos_theta / total_mass));
        const x_acc = temp - pole_mass_length * theta_acc * cos_theta / total_mass;

        // Euler integration
        self.state[0] = x + tau * x_dot;
        self.state[1] = x_dot + tau * x_acc;
        self.state[2] = theta + tau * theta_dot;
        self.state[3] = theta_dot + tau * theta_acc;

        // Check termination
        const done = self.state[0] < -2.4 or self.state[0] > 2.4 or
            self.state[2] < -0.21 or self.state[2] > 0.21;

        // Reward shaping: penalize angle and position deviation
        var reward: f32 = 1.0;
        if (done) {
            reward = -10.0; // Strong penalty for failure
        } else {
            // Bonus for keeping pole upright and cart centered
            reward += 1.0 - @abs(self.state[2]) / 0.21; // Angle bonus
            reward += 0.5 - @abs(self.state[0]) / 2.4;  // Position bonus
        }

        return .{ .state = self.state, .reward = reward, .done = done };
    }
};

// ============================================================================
// TRAINING
// ============================================================================

pub fn trainCartPole(n_episodes: u32) !void {
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n", .{});
    print("â•‘     HDC DOUBLE Q-LEARNING - CARTPOLE-v1                      â•‘\n", .{});
    print("â•‘     Ï†Â² + 1/Ï†Â² = 3 | TRINITY | D={d}                        â•‘\n", .{DIMENSION});
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n", .{});

    print("Training HDC Double Q-Learning agent on CartPole...\n", .{});
    print("Params: D={d}, lr=0.1, gamma=0.99, Îµ_decay=0.995\n", .{DIMENSION});
    print("Target: â‰¥195 average reward over 100 episodes\n\n", .{});

    var env = CartPoleEnv.init(42);
    var agent = HDCCartPoleAgent.init(42);

    var total_rewards: [100]f32 = [_]f32{0.0} ** 100;
    var idx: usize = 0;
    var best_avg: f32 = 0.0;
    var solved = false;
    var solved_episode: u32 = 0;

    for (0..n_episodes) |episode| {
        _ = env.reset();
        var episode_reward: f32 = 0.0;

        for (0..500) |_| { // Max 500 steps
            const state = env.state;
            const action = agent.chooseAction(state);
            const result = env.step(action);

            agent.tdUpdate(state, action, result.reward, result.state, result.done);
            episode_reward += result.reward;

            if (result.done) break;
        }

        agent.decayEpsilon();

        total_rewards[idx] = episode_reward;
        idx = (idx + 1) % 100;

        // Calculate running average
        var sum: f32 = 0.0;
        for (total_rewards) |r| {
            sum += r;
        }
        const avg = sum / 100.0;

        if (avg > best_avg) {
            best_avg = avg;
        }

        if (!solved and avg >= 195.0 and episode >= 100) {
            solved = true;
            solved_episode = @intCast(episode);
            print("ðŸŽ¯ SOLVED at episode {d}! Avg reward: {d:.1}\n", .{ episode, avg });
        }

        if ((episode + 1) % 100 == 0) {
            print("Episode {d:5} | Reward: {d:5.0} | Avg(100): {d:5.1} | Best: {d:5.1} | Îµ: {d:.3}\n", .{ episode + 1, episode_reward, avg, best_avg, agent.epsilon });
        }
    }

    // Final average
    var final_sum: f32 = 0.0;
    for (total_rewards) |r| {
        final_sum += r;
    }
    const final_avg = final_sum / 100.0;

    print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    print("                    FINAL RESULTS\n", .{});
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    print("  Dimension:          {d}\n", .{DIMENSION});
    print("  Episodes:           {d}\n", .{n_episodes});
    print("  Final Avg (100):    {d:.1}\n", .{final_avg});
    print("  Best Avg:           {d:.1}\n", .{best_avg});

    if (solved) {
        print("  Solved at:          Episode {d}\n", .{solved_episode});
        print("\nðŸ† CARTPOLE SOLVED! â‰¥195 average reward! ðŸ†\n", .{});
    } else if (final_avg >= 150.0) {
        print("\nâœ… Good progress: {d:.1} average reward\n", .{final_avg});
    } else {
        print("\nðŸ“Š Average reward: {d:.1}\n", .{final_avg});
    }
}

pub fn main() !void {
    try trainCartPole(2000);
}

test "cartpole_env" {
    var env = CartPoleEnv.init(42);
    const state = env.reset();
    try std.testing.expect(state[0] >= -0.05 and state[0] <= 0.05);

    const result = env.step(1);
    try std.testing.expect(!result.done or result.done);
}

test "continuous_encoder" {
    var prng = std.Random.DefaultPrng.init(42);
    var rng = prng.random();
    const encoder = ContinuousStateEncoder.init(&rng);

    const state1 = [4]f32{ 0.0, 0.0, 0.0, 0.0 };
    const state2 = [4]f32{ 0.1, 0.0, 0.0, 0.0 };

    const hv1 = encoder.encode(state1);
    const hv2 = encoder.encode(state2);

    // Similar states should have similar encodings
    const sim = hv1.dot(hv2) / @as(f32, DIMENSION * DIMENSION);
    try std.testing.expect(sim > 0.0);
}
