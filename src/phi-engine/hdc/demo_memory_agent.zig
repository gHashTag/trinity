//! Demo Memory Agent - ÐÐ³ÐµÐ½Ñ‚ Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð² GridWorld
//!
//! Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ RL Ð°Ð³ÐµÐ½Ñ‚Ð° Ñ Streaming Memory Ð´Ð»Ñ experience replay.
//! Ð¦ÐµÐ»ÑŒ: Ð´Ð¾ÑÑ‚Ð¸Ñ‡ÑŒ 100% win rate Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸.
//!
//! Ï†Â² + 1/Ï†Â² = 3 | TRINITY

const std = @import("std");
const hdc = @import("hdc_core.zig");
const gw = @import("gridworld.zig");
const rlm = @import("rl_agent_memory.zig");

const print = std.debug.print;

/// ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð´ÐµÐ¼Ð¾
const DemoConfig = struct {
    grid_size: usize = 4,
    num_episodes: usize = 1000,
    state_dim: usize = 256,
    memory_dim: usize = 2000,
    learning_rate: f64 = 0.2,
    gamma: f64 = 0.95,
    epsilon_start: f64 = 1.0,
    epsilon_end: f64 = 0.01,
    epsilon_decay: f64 = 0.99,
    print_every: usize = 100,
    render_final: bool = true,
};

/// Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð´ÐµÐ¼Ð¾
pub fn runDemo(allocator: std.mem.Allocator, config: DemoConfig) !void {
    print("\n", .{});
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n", .{});
    print("â•‘     TRINITY HDC RL AGENT WITH MEMORY - GRIDWORLD             â•‘\n", .{});
    print("â•‘     Ï†Â² + 1/Ï†Â² = 3                                            â•‘\n", .{});
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    print("\n", .{});

    // Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÑ€ÐµÐ´Ñƒ
    var env = try gw.GridWorld.init(allocator, .{
        .width = config.grid_size,
        .height = config.grid_size,
        .step_reward = -0.1,
        .goal_reward = 10.0,
        .max_steps = config.grid_size * config.grid_size * 2,
    });
    defer env.deinit();

    print("Ð¡Ñ€ÐµÐ´Ð°: GridWorld {d}x{d}\n", .{ config.grid_size, config.grid_size });
    print("Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹: {d}, Ð”ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹: {d}\n", .{ env.numStates(), gw.NUM_ACTIONS });
    print("\n", .{});

    // Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð°Ð³ÐµÐ½Ñ‚Ð° Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ
    var agent = try rlm.RLAgentWithMemory.init(allocator, .{
        .state_dim = config.state_dim,
        .num_actions = gw.NUM_ACTIONS,
        .num_states = env.numStates(),
        .gamma = config.gamma,
        .learning_rate = config.learning_rate,
        .epsilon_start = config.epsilon_start,
        .epsilon_end = config.epsilon_end,
        .epsilon_decay = config.epsilon_decay,
        .memory_dim = config.memory_dim,
    });
    defer agent.deinit();

    print("ÐÐ³ÐµÐ½Ñ‚: HDC RL Ñ Streaming Memory\n", .{});
    print("ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹: Î³={d:.2}, Î±={d:.2}, memory_dim={d}\n", .{
        config.gamma,
        config.learning_rate,
        config.memory_dim,
    });
    print("\n", .{});

    print("ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ({d} ÑÐ¿Ð¸Ð·Ð¾Ð´Ð¾Ð²)...\n", .{config.num_episodes});
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n", .{});

    var total_steps: u64 = 0;
    var wins: u64 = 0;
    var recent_rewards: [100]f64 = [_]f64{0} ** 100;
    var recent_idx: usize = 0;
    var consecutive_wins: u64 = 0;
    var max_consecutive_wins: u64 = 0;

    const start_time = std.time.milliTimestamp();

    for (0..config.num_episodes) |episode| {
        var state = env.reset();
        var episode_reward: f64 = 0;
        var episode_steps: usize = 0;

        while (true) {
            const action = agent.selectAction(state);
            const result = env.step(action);

            const exp = rlm.Experience{
                .state_id = state,
                .action_id = action,
                .reward = result.reward,
                .next_state_id = result.next_state,
                .done = result.done,
            };

            _ = try agent.learnWithReplay(exp);

            episode_reward += result.reward;
            episode_steps += 1;
            state = result.next_state;

            if (result.done) {
                if (std.mem.eql(u8, result.info, "goal")) {
                    wins += 1;
                    consecutive_wins += 1;
                    if (consecutive_wins > max_consecutive_wins) {
                        max_consecutive_wins = consecutive_wins;
                    }
                } else {
                    consecutive_wins = 0;
                }
                break;
            }
        }

        agent.endEpisode(episode_reward);
        total_steps += episode_steps;

        recent_rewards[recent_idx] = episode_reward;
        recent_idx = (recent_idx + 1) % 100;

        if ((episode + 1) % config.print_every == 0) {
            var avg_reward: f64 = 0;
            const count = @min(episode + 1, 100);
            for (0..count) |i| {
                avg_reward += recent_rewards[i];
            }
            avg_reward /= @as(f64, @floatFromInt(count));

            const win_rate = @as(f64, @floatFromInt(wins)) / @as(f64, @floatFromInt(episode + 1)) * 100;

            print("Ð­Ð¿Ð¸Ð·Ð¾Ð´ {d:4}: avg={d:6.2}, win={d:5.1}%, Îµ={d:.3}, mem={d}\n", .{
                episode + 1,
                avg_reward,
                win_rate,
                agent.getEpsilon(),
                agent.experience_count,
            });
        }
    }

    const end_time = std.time.milliTimestamp();
    const duration_ms = end_time - start_time;

    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n", .{});
    print("\n", .{});

    const metrics = agent.getMetrics();
    const mem_metrics = agent.getMemoryMetrics();
    const final_win_rate = @as(f64, @floatFromInt(wins)) / @as(f64, @floatFromInt(config.num_episodes)) * 100;

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n", .{});
    print("â•‘                    Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð¯                       â•‘\n", .{});
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n", .{});
    print("â•‘ Ð­Ð¿Ð¸Ð·Ð¾Ð´Ð¾Ð²:           {d:6}                                    â•‘\n", .{config.num_episodes});
    print("â•‘ Ð’ÑÐµÐ³Ð¾ ÑˆÐ°Ð³Ð¾Ð²:        {d:6}                                    â•‘\n", .{total_steps});
    print("â•‘ ÐŸÐ¾Ð±ÐµÐ´:              {d:6} ({d:.1}%)                           â•‘\n", .{ wins, final_win_rate });
    print("â•‘ Max consecutive:    {d:6}                                    â•‘\n", .{max_consecutive_wins});
    print("â•‘ Avg reward (100):   {d:7.2}                                   â•‘\n", .{metrics.avg_reward_100});
    print("â•‘ Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Îµ:        {d:6.4}                                   â•‘\n", .{agent.getEpsilon()});
    print("â•‘ ÐžÐ¿Ñ‹Ñ‚Ð¾Ð² Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸:    {d:6}                                    â•‘\n", .{mem_metrics.total_writes});
    print("â•‘ Ð’Ñ€ÐµÐ¼Ñ:              {d:6} ms                                 â•‘\n", .{duration_ms});
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});

    // Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ
    if (config.render_final) {
        print("\n", .{});
        print("Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð° (greedy policy):\n", .{});
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n", .{});

        var demo_state = env.reset();
        env.render();

        for (0..20) |step_num| {
            const demo_action = agent.selectActionGreedy(demo_state);

            print("Ð¨Ð°Ð³ {d}: Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ = {s}\n", .{ step_num + 1, @as(gw.Action, @enumFromInt(demo_action)).toString() });

            const demo_result = env.step(demo_action);
            env.render();

            if (demo_result.done) {
                if (std.mem.eql(u8, demo_result.info, "goal")) {
                    print("\nâœ… Ð¦Ð•Ð›Ð¬ Ð”ÐžÐ¡Ð¢Ð˜Ð“ÐÐ£Ð¢Ð Ð·Ð° {d} ÑˆÐ°Ð³Ð¾Ð²!\n", .{step_num + 1});
                } else {
                    print("\nâš ï¸ Ð­Ð¿Ð¸Ð·Ð¾Ð´ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½: {s}\n", .{demo_result.info});
                }
                break;
            }

            demo_state = demo_result.next_state;
        }
    }

    print("\n", .{});
    if (final_win_rate >= 99.0) {
        print("ðŸ† ÐœÐ˜Ð¡Ð¡Ð˜Ð¯ Ð’Ð«ÐŸÐžÐ›ÐÐ•ÐÐ: {d:.1}% WIN RATE!\n", .{final_win_rate});
    } else if (final_win_rate >= 95.0) {
        print("âœ… ÐžÐ¢Ð›Ð˜Ð§ÐÐ«Ð™ Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢: {d:.1}% WIN RATE\n", .{final_win_rate});
    } else {
        print("âš ï¸ Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð´Ð¾Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°: {d:.1}% WIN RATE\n", .{final_win_rate});
    }
    print("\n", .{});
    print("Ï†Â² + 1/Ï†Â² = 3 | TRINITY HDC RL WITH MEMORY COMPLETE\n", .{});
}

/// Ð¢Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð° (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»Ð½ÑÐµÐ¼Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð°)
pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    try runDemo(allocator, .{
        .grid_size = 4,
        .num_episodes = 1000,
        .print_every = 200,
    });
}

// ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ main Ð¿Ñ€Ð¸ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸
comptime {
    if (@import("builtin").is_test) {
        _ = main;
    }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Ð¢Ð•Ð¡Ð¢Ð«
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

test "agent with memory learns" {
    const allocator = std.testing.allocator;

    // Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÑ€ÐµÐ´Ñƒ
    var env = try gw.GridWorld.init(allocator, .{
        .width = 2,
        .height = 2,
    });
    defer env.deinit();

    // Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð°Ð³ÐµÐ½Ñ‚Ð°
    var agent = try rlm.RLAgentWithMemory.init(allocator, .{
        .num_states = 4,
        .num_actions = 4,
        .epsilon_start = 0.5,
        .epsilon_decay = 0.9,
    });
    defer agent.deinit();

    // ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ 50 ÑÐ¿Ð¸Ð·Ð¾Ð´Ð¾Ð²
    var wins: u32 = 0;
    for (0..50) |_| {
        var state = env.reset();
        while (true) {
            const action = agent.selectAction(state);
            const result = env.step(action);

            const exp = rlm.Experience{
                .state_id = state,
                .action_id = action,
                .reward = result.reward,
                .next_state_id = result.next_state,
                .done = result.done,
            };
            _ = try agent.learnWithReplay(exp);

            state = result.next_state;
            if (result.done) {
                if (std.mem.eql(u8, result.info, "goal")) wins += 1;
                break;
            }
        }
        agent.decayEpsilon();
    }

    // Ð”Ð¾Ð»Ð¶ÐµÐ½ Ð²Ñ‹Ð¸Ð³Ñ€Ð°Ñ‚ÑŒ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ 30% (2x2 grid Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹)
    try std.testing.expect(wins > 15);
}
