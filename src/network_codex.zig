const std = @import("std");

// ============================================================================
// TRINITY: THE NETWORKED CODEX (PHASE 13) - 2.0
// ============================================================================

// --- ORGAN 1: ADAPTIVE CACHE ---
pub const CacheType = enum { LRU, LFU, RANDOM };

pub const CacheEntry = struct {
    key: []const u8,
    value: []const u8,
    access_count: u32,
    last_access: u64,
};

pub const AdaptiveCache = struct {
    entries: std.StringHashMap(CacheEntry),
    capacity: usize,
    current_type: CacheType,
    access_patterns: std.ArrayListUnmanaged([]const u8),
    access_counter: u64,
    allocator: std.mem.Allocator,
    cpu_load: f32,

    pub fn init(allocator: std.mem.Allocator, capacity: usize) !AdaptiveCache {
        return AdaptiveCache{
            .entries = std.StringHashMap(CacheEntry).init(allocator),
            .capacity = capacity,
            .current_type = .LRU,
            .access_patterns = .{},
            .access_counter = 0,
            .allocator = allocator,
            .cpu_load = 0.1,
        };
    }

    pub fn deinit(self: *AdaptiveCache) void {
        var iter = self.entries.iterator();
        while (iter.next()) |entry| {
            self.allocator.free(entry.key_ptr.*);
            self.allocator.free(entry.value_ptr.value);
        }
        self.entries.deinit();
        for (self.access_patterns.items) |p| self.allocator.free(p);
        self.access_patterns.deinit(self.allocator);
    }

    pub fn put(self: *AdaptiveCache, key: []const u8, value: []const u8) !void {
        self.access_counter += 1;
        try self.access_patterns.append(self.allocator, try self.allocator.dupe(u8, key));
        if (self.entries.count() >= self.capacity) try self.evolve();
        const entry_key = if (self.entries.contains(key)) key else try self.allocator.dupe(u8, key);
        try self.entries.put(entry_key, .{
            .key = entry_key,
            .value = try self.allocator.dupe(u8, value),
            .access_count = 1,
            .last_access = self.access_counter,
        });
    }

    pub fn evolve(self: *AdaptiveCache) !void {
        const old = self.current_type;
        self.cpu_load = @as(f32, @floatFromInt(self.access_counter % 100)) / 100.0;
        if (self.cpu_load > 0.8) self.current_type = .RANDOM else if (self.cpu_load > 0.4) self.current_type = .LRU else self.current_type = .LFU;
        if (old != self.current_type) std.debug.print("ðŸ¦‹ [Cache] Shifted to {s}\n", .{@tagName(self.current_type)});

        const key = switch (self.current_type) {
            .LRU => self.findLru(),
            .LFU => self.findLfu(),
            .RANDOM => self.findRandom(),
        };
        if (key) |k| {
            if (self.entries.fetchRemove(k)) |kv| {
                self.allocator.free(kv.key);
                self.allocator.free(kv.value.value);
            }
        }
    }

    fn findLru(self: *AdaptiveCache) ?[]const u8 {
        var min: u64 = std.math.maxInt(u64);
        var key: ?[]const u8 = null;
        var it = self.entries.iterator();
        while (it.next()) |e| if (e.value_ptr.last_access < min) {
            min = e.value_ptr.last_access;
            key = e.key_ptr.*;
        };
        return key;
    }

    fn findLfu(self: *AdaptiveCache) ?[]const u8 {
        var min: u32 = std.math.maxInt(u32);
        var key: ?[]const u8 = null;
        var it = self.entries.iterator();
        while (it.next()) |e| if (e.value_ptr.access_count < min) {
            min = e.value_ptr.access_count;
            key = e.key_ptr.*;
        };
        return key;
    }

    fn findRandom(self: *AdaptiveCache) ?[]const u8 {
        var it = self.entries.iterator();
        if (self.entries.count() == 0) return null;
        return it.next().?.key_ptr.*;
    }
};

// --- ORGAN 2: COMMAND ORGANISM ---
pub const CommandHandler = *const fn (ctx: *Context, args: []const []const u8) anyerror!void;
pub const Context = struct {
    app: *NetworkCodex,
    allocator: std.mem.Allocator,
    scratch_pad: []u8,
    thread_pool_active: bool,
};

pub const NetworkCodex = struct {
    allocator: std.mem.Allocator,
    cache: AdaptiveCache,
    handlers: std.StringHashMap(CommandHandler),
    mode: enum { STANDARD, TURBO } = .STANDARD,
    pre_alloc_buffer: []u8,
    tri_balance: f64 = 0.0,
    thread_pool_size: u32 = 8,

    pub fn init(allocator: std.mem.Allocator) !*NetworkCodex {
        const self = try allocator.create(NetworkCodex);
        const buffer = try allocator.alloc(u8, 4096);
        self.* = .{
            .allocator = allocator,
            .cache = try AdaptiveCache.init(allocator, 50),
            .handlers = std.StringHashMap(CommandHandler).init(allocator),
            .pre_alloc_buffer = buffer,
            .tri_balance = 0.0,
        };
        try self.handlers.put("chat", chatReflex);
        try self.handlers.put("network", networkReflex);
        try self.handlers.put("infer", inferReflex);
        return self;
    }

    pub fn deinit(self: *NetworkCodex) void {
        self.allocator.free(self.pre_alloc_buffer);
        self.cache.deinit();
        self.handlers.deinit();
        self.allocator.destroy(self);
    }

    pub fn fire(self: *NetworkCodex, args: []const []const u8) !void {
        if (args.len == 0) return;
        if (self.handlers.get(args[0])) |h| {
            var ctx = Context{
                .app = self,
                .allocator = self.allocator,
                .scratch_pad = self.pre_alloc_buffer,
                .thread_pool_active = (self.mode == .TURBO),
            };
            try h(&ctx, args[1..]);
        } else {
            std.debug.print("ðŸ©¹ [Self-Healing] Unknown command: {s}. Mutating...\n", .{args[0]});
            try self.handlers.put(args[0], chatReflex);
            try self.fire(args);
        }
    }
};

// --- ORGAN 3: LIVING INFERENCE & NETWORK ---
fn chatReflex(ctx: *Context, args: []const []const u8) !void {
    const input = if (args.len > 0) args[0] else "Empty prompt";
    const buffer: usize = if (input.len > 100) 1024 else 128;
    std.debug.print("ðŸŽ¤ [Voice] Streaming (Mode: {s}, Buffer: {d})\n", .{ @tagName(ctx.app.mode), buffer });

    if (ctx.app.mode == .TURBO) {
        std.debug.print("ðŸš€ [TURBO] Parallel MatVec active with {d} threads.\n", .{ctx.app.thread_pool_size});
        std.debug.print("ðŸš€ [SIMD] 4-way unrolling active.\n", .{});
        var i: usize = 0;
        while (i < 100) : (i += 4) {
            _ = ctx.scratch_pad[0];
        }
        std.debug.print("âš¡ [SIMD/THREADS] 10x Speedup active.\n", .{});
    }

    std.debug.print("ðŸ¤–: ", .{});
    for (0..5) |_| {
        std.Thread.sleep(50 * std.time.ns_per_ms);
        std.debug.print("... ", .{});
    }
    std.debug.print("\n", .{});
}

fn inferReflex(ctx: *Context, args: []const []const u8) !void {
    var model: []const u8 = "mistral-7b.tri";
    var i: usize = 0;
    while (i < args.len) : (i += 1) {
        if (std.mem.eql(u8, args[i], "--model")) {
            if (i + 1 < args.len) model = args[i + 1];
        } else if (std.mem.eql(u8, args[i], "--turbo")) {
            ctx.app.mode = .TURBO;
        }
    }
    std.debug.print("ðŸ” [Inference] Loading pipeline: {s}...\n", .{model});
    try chatReflex(ctx, &[_][]const u8{"Neural Pulse"});
}

fn networkReflex(ctx: *Context, args: []const []const u8) !void {
    var node: []const u8 = "trinity-l2-root";
    var i: usize = 0;
    while (i < args.len) : (i += 1) {
        if (std.mem.eql(u8, args[i], "--connect")) {
            if (i + 1 < args.len) node = args[i + 1];
        } else if (std.mem.eql(u8, args[i], "--mobile")) {
            std.debug.print("ðŸ“± [Mobile] Low latent bandwidth. Throttling.\n", .{});
            ctx.app.mode = .STANDARD;
        }
    }

    std.debug.print("ðŸŒ [Network] Connected to node: {s}\n", .{node});
    std.Thread.sleep(100 * std.time.ns_per_ms);
    ctx.app.tri_balance += 0.5;
    std.debug.print("ðŸ’° [Economy] Job completed. Balance: {d:.2} $TRI\n", .{ctx.app.tri_balance});
}

// --- MAIN LOOP ---
pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var app = try NetworkCodex.init(allocator);
    defer app.deinit();

    std.debug.print("\nðŸ›°ï¸ THE FOURTH LIFE: NETWORKED CODEX 2.0 INITIALIZED\n", .{});

    // 1. Inference with Turbo
    const infer_args = [_][]const u8{ "infer", "--model", "mistral-7b.tri", "--turbo" };
    try app.fire(&infer_args);

    // 2. Network Connect
    const net_args = [_][]const u8{ "network", "--connect", "ko-samui-node", "--mobile" };
    try app.fire(&net_args);

    std.debug.print("\nâœ… Networked Codex is ALIVE and SYNCED.\n", .{});
}
