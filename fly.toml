# TRINITY LLM - Fly.io Configuration
# phi^2 + 1/phi^2 = 3 = TRINITY

app = "trinity-llm"
primary_region = "iad"

[build]
  dockerfile = "Dockerfile"

[env]
  MODEL_PATH = "/app/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf"
  TEMPERATURE = "0.7"
  TOP_P = "0.9"

# Use performance-2x for TinyLlama (2 CPU, 4GB RAM)
[[vm]]
  size = "performance-2x"
  memory = "4gb"
  cpus = 2

# Persistent volume for models (optional - model is baked into image)
# [[mounts]]
#   source = "trinity_models"
#   destination = "/app/models"

# No HTTP service - this is a CLI application
# Access via: fly ssh console -a trinity-llm
# Then run: /app/vibee chat --model /app/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf

[processes]
  app = "/bin/sleep infinity"
