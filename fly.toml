# TRINITY LLM - Fly.io Configuration
# phi^2 + 1/phi^2 = 3 = TRINITY

app = "trinity-llm"
primary_region = "iad"

[build]
  dockerfile = "Dockerfile"

[env]
  MODEL_PATH = "/app/models/smollm-135m-instruct-q8_0.gguf"
  TEMPERATURE = "0.7"
  TOP_P = "0.9"

# Use shared-cpu-1x for SmolLM-135M (small model)
# 1GB RAM needed for model loading + inference buffers
[[vm]]
  size = "shared-cpu-1x"
  memory = "1gb"
  cpus = 1

# Persistent volume for models (optional - model is baked into image)
# [[mounts]]
#   source = "trinity_models"
#   destination = "/app/models"

# HTTP API service
[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0

[[http_service.checks]]
  grace_period = "120s"
  interval = "30s"
  method = "GET"
  path = "/health"
  timeout = "15s"
