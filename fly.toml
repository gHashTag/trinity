# ═══════════════════════════════════════════════════════════════════════════════
# TRINITY LLM - Fly.io Configuration
# Multi-threaded LLM inference with MAXIMUM CPU cores
# φ² + 1/φ² = 3 = TRINITY
# ═══════════════════════════════════════════════════════════════════════════════

app = "trinity-llm"
primary_region = "iad"

[build]
  dockerfile = "Dockerfile"

[env]
  MODEL_PATH = "/app/models/smollm2-1.7b-instruct-q8_0.gguf"
  TEMPERATURE = "0.7"
  TOP_P = "0.9"
  NUM_THREADS = "16"

# SmolLM2-1.7B requires more RAM (~4GB for model + buffers)
# performance-4x: 4 dedicated CPU cores, 8GB RAM
[[vm]]
  size = "performance-16x"
  memory = "32gb"
  cpus = 16

# Alternative sizes:
# performance-8x: 8 CPU, 16GB RAM (faster, more expensive)
# performance-16x: 16 CPU, 32GB RAM (maximum speed)
# shared-cpu-4x: 4 shared CPU, 8GB RAM (cheaper)

# Persistent volume for models
# [[mounts]]
#   source = "trinity_models"
#   destination = "/app/models"

# HTTP API service
[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = "off"
  auto_start_machines = true
  min_machines_running = 1

[[http_service.checks]]
  grace_period = "180s"  # SmolLM2-1.7B needs ~30-60s to load
  interval = "30s"
  method = "GET"
  path = "/health"
  timeout = "30s"
