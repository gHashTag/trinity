# TRINITY LLM - Fly.io Configuration
# phi^2 + 1/phi^2 = 3 = TRINITY

app = "trinity-llm"
primary_region = "iad"

[build]
  dockerfile = "Dockerfile"

[env]
  MODEL_PATH = "/app/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf"
  TEMPERATURE = "0.7"
  TOP_P = "0.9"

# Use performance-2x for TinyLlama (2 CPU, 4GB RAM)
[[vm]]
  size = "performance-2x"
  memory = "4gb"
  cpus = 2

# Persistent volume for models (optional - model is baked into image)
# [[mounts]]
#   source = "trinity_models"
#   destination = "/app/models"

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0

[[http_service.checks]]
  grace_period = "120s"
  interval = "30s"
  method = "GET"
  path = "/health"
  timeout = "10s"
