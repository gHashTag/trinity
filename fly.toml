# ═══════════════════════════════════════════════════════════════════════════════
# TRINITY LLM - Fly.io Configuration
# Multi-threaded LLM inference with MAXIMUM CPU cores
# φ² + 1/φ² = 3 = TRINITY
# ═══════════════════════════════════════════════════════════════════════════════

app = "trinity-llm"
primary_region = "iad"

[build]
  dockerfile = "Dockerfile"

[env]
  MODEL_PATH = "/app/models/smollm-135m-instruct-q8_0.gguf"
  TEMPERATURE = "0.7"
  TOP_P = "0.9"
  NUM_THREADS = "16"

# MAXIMUM CPU: performance-16x = 16 dedicated CPU cores, 32GB RAM
# For benchmark testing multi-threaded inference
[[vm]]
  size = "performance-16x"
  memory = "32gb"
  cpus = 16

# Alternative sizes:
# performance-8x: 8 CPU, 16GB RAM
# performance-4x: 4 CPU, 8GB RAM
# shared-cpu-8x: 8 shared CPU, 16GB RAM

# Persistent volume for models
# [[mounts]]
#   source = "trinity_models"
#   destination = "/app/models"

# HTTP API service
[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0

[[http_service.checks]]
  grace_period = "120s"
  interval = "30s"
  method = "GET"
  path = "/health"
  timeout = "15s"
