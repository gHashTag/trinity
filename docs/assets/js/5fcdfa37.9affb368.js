"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[3270],{1601(e,n,r){r.r(n),r.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>o,frontMatter:()=>d,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"benchmarks/gpu-inference","title":"GPU Inference Benchmarks","description":"BitNet b1.58 models use ternary weights (\\\\{-1, 0, +1\\\\}), enabling highly efficient inference on both consumer and datacenter GPUs. This page summarizes performance measurements across different hardware configurations.","source":"@site/docs/benchmarks/gpu-inference.md","sourceDirName":"benchmarks","slug":"/benchmarks/gpu-inference","permalink":"/trinity/docs/benchmarks/gpu-inference","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/benchmarks/gpu-inference.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Benchmarks","permalink":"/trinity/docs/benchmarks/"},"next":{"title":"JIT Compilation Performance","permalink":"/trinity/docs/benchmarks/jit-performance"}}');var i=r(4848),s=r(8453);const d={sidebar_position:2},c="GPU Inference Benchmarks",h={},l=[{value:"Hardware Comparison",id:"hardware-comparison",level:2},{value:"Model Size Scaling",id:"model-size-scaling",level:2},{value:"Batch Size Effects",id:"batch-size-effects",level:2},{value:"Memory Requirements",id:"memory-requirements",level:2},{value:"Inference Frameworks",id:"inference-frameworks",level:2}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"gpu-inference-benchmarks",children:"GPU Inference Benchmarks"})}),"\n",(0,i.jsx)(n.p,{children:"BitNet b1.58 models use ternary weights ({-1, 0, +1}), enabling highly efficient inference on both consumer and datacenter GPUs. This page summarizes performance measurements across different hardware configurations."}),"\n",(0,i.jsx)(n.h2,{id:"hardware-comparison",children:"Hardware Comparison"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"GPU"}),(0,i.jsx)(n.th,{children:"Tokens/sec (eval)"}),(0,i.jsx)(n.th,{children:"Tokens/sec (prompt)"}),(0,i.jsx)(n.th,{children:"Memory Usage"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"NVIDIA RTX 3090"}),(0,i.jsx)(n.td,{children:"~298,000"}),(0,i.jsx)(n.td,{children:"~350,000"}),(0,i.jsx)(n.td,{children:"~1.3 GB"}),(0,i.jsx)(n.td,{children:"Consumer GPU, 24GB VRAM"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"NVIDIA A100 80GB"}),(0,i.jsx)(n.td,{children:"~274,000"}),(0,i.jsx)(n.td,{children:"~320,000"}),(0,i.jsx)(n.td,{children:"~1.3 GB"}),(0,i.jsx)(n.td,{children:"Datacenter GPU, PCIe/SXM"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"NVIDIA H100 SXM"}),(0,i.jsx)(n.td,{children:"~300,000+"}),(0,i.jsx)(n.td,{children:"~380,000"}),(0,i.jsx)(n.td,{children:"~1.3 GB"}),(0,i.jsx)(n.td,{children:"Datacenter, AVX-512 VNNI on CPU side"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CPU-only (M1 Pro)"}),(0,i.jsx)(n.td,{children:"~0.2"}),(0,i.jsx)(n.td,{children:"N/A"}),(0,i.jsx)(n.td,{children:"~4 GB"}),(0,i.jsx)(n.td,{children:"ARM64, no GPU acceleration"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CPU-only (x86 AVX-512)"}),(0,i.jsx)(n.td,{children:"~15,000"}),(0,i.jsx)(n.td,{children:"~18,000"}),(0,i.jsx)(n.td,{children:"~1.3 GB"}),(0,i.jsx)(n.td,{children:"Server CPU with AVX-512 VNNI"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"The numbers above are for the BitNet b1.58-2B-4T model (2.4 billion parameters) using the bitnet.cpp inference engine with I2_S quantization. Actual throughput depends on batch size, sequence length, and system configuration."}),"\n",(0,i.jsx)(n.admonition,{type:"caution",children:(0,i.jsxs)(n.p,{children:["These throughput figures represent bitnet.cpp kernel benchmark results (measuring raw computation speed), not end-to-end text generation throughput. End-to-end generation speed is substantially lower due to sequential token generation, memory transfers, and tokenizer overhead. See the ",(0,i.jsx)(n.a,{href:"/docs/research/bitnet-report",children:"BitNet Coherence Report"})," for measured end-to-end generation speeds."]})}),"\n",(0,i.jsx)(n.h2,{id:"model-size-scaling",children:"Model Size Scaling"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Parameters"}),(0,i.jsx)(n.th,{children:"GGUF Size"}),(0,i.jsx)(n.th,{children:"Min VRAM"}),(0,i.jsx)(n.th,{children:"Approx. Throughput (RTX 3090)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"BitNet b1.58 Small"}),(0,i.jsx)(n.td,{children:"~700M"}),(0,i.jsx)(n.td,{children:"~350 MB"}),(0,i.jsx)(n.td,{children:"1 GB"}),(0,i.jsx)(n.td,{children:"~400K tok/s"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"BitNet b1.58-2B-4T"}),(0,i.jsx)(n.td,{children:"2.4B"}),(0,i.jsx)(n.td,{children:"1.1 GB"}),(0,i.jsx)(n.td,{children:"2 GB"}),(0,i.jsx)(n.td,{children:"~298K tok/s"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"BitNet b1.58 3B"}),(0,i.jsx)(n.td,{children:"~3B"}),(0,i.jsx)(n.td,{children:"~1.4 GB"}),(0,i.jsx)(n.td,{children:"2 GB"}),(0,i.jsx)(n.td,{children:"~220K tok/s"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"BitNet b1.58 7B"}),(0,i.jsx)(n.td,{children:"~7B"}),(0,i.jsx)(n.td,{children:"~3.2 GB"}),(0,i.jsx)(n.td,{children:"4 GB"}),(0,i.jsx)(n.td,{children:"~95K tok/s"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Ternary quantization (I2_S) produces model files that are approximately 20x smaller than their float32 equivalents. A 7B parameter model that would normally require ~28 GB in float32 fits in roughly 3.2 GB with ternary weights."}),"\n",(0,i.jsx)(n.h2,{id:"batch-size-effects",children:"Batch Size Effects"}),"\n",(0,i.jsx)(n.p,{children:"Batch size has a significant impact on throughput. Single-token generation (batch size 1) is latency-optimized, while larger batch sizes improve aggregate throughput at the cost of per-token latency."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Batch Size"}),(0,i.jsx)(n.th,{children:"Throughput Multiplier"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1"}),(0,i.jsx)(n.td,{children:"1x (baseline)"}),(0,i.jsx)(n.td,{children:"Interactive chat, real-time generation"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4"}),(0,i.jsx)(n.td,{children:"~2.5x"}),(0,i.jsx)(n.td,{children:"Small batch serving"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"16"}),(0,i.jsx)(n.td,{children:"~6x"}),(0,i.jsx)(n.td,{children:"Batch processing"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"64"}),(0,i.jsx)(n.td,{children:"~12x"}),(0,i.jsx)(n.td,{children:"Offline processing, benchmarks"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"memory-requirements",children:"Memory Requirements"}),"\n",(0,i.jsx)(n.p,{children:"The ternary weight format dramatically reduces memory consumption:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model weights"}),": 1.58 bits per parameter (vs 32 bits for float32, 16 bits for float16)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"KV cache"}),": Standard float16, scales with context length and batch size"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Activations"}),": 8-bit quantized activations further reduce memory during inference"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"For the 2B-4T model, peak memory usage is approximately 1.3 GB for the model weights plus KV cache overhead that scales with sequence length. A 4096-token context window adds roughly 200-400 MB depending on the number of KV heads."}),"\n",(0,i.jsx)(n.h2,{id:"inference-frameworks",children:"Inference Frameworks"}),"\n",(0,i.jsx)(n.p,{children:"Trinity's benchmarks use the official Microsoft bitnet.cpp framework, which provides optimized kernels for ternary inference:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"I2_S kernel"}),": Optimized for ternary weight unpacking and accumulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TL2 kernel"}),": Advanced kernel with tiling for better cache utilization on x86"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ARM NEON"}),": Vectorized path for Apple Silicon and ARM servers"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The RunPod deployment script (",(0,i.jsx)(n.code,{children:"scripts/runpod_h100_bitnet.sh"}),") automates benchmarking across thread counts and prompt variations on cloud GPU instances."]})]})}function o(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453(e,n,r){r.d(n,{R:()=>d,x:()=>c});var t=r(6540);const i={},s=t.createContext(i);function d(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);