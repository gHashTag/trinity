"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[6630],{4242(e,t,i){i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"bitnet/index","title":"BitNet Integration","description":"What is BitNet b1.58?","source":"@site/docs/bitnet/index.md","sourceDirName":"bitnet","slug":"/bitnet/","permalink":"/trinity/docs/bitnet/","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/bitnet/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Glossary","permalink":"/trinity/docs/concepts/glossary"},"next":{"title":"Inference Pipeline","permalink":"/trinity/docs/bitnet/inference"}}');var r=i(4848),a=i(8453);const o={sidebar_position:1},s="BitNet Integration",l={},c=[{value:"What is BitNet b1.58?",id:"what-is-bitnet-b158",level:2},{value:"Why Trinity Implements BitNet Natively",id:"why-trinity-implements-bitnet-natively",level:2},{value:"Inference Pipeline Overview",id:"inference-pipeline-overview",level:2},{value:"Research Foundation",id:"research-foundation",level:2}];function d(e){const t={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"bitnet-integration",children:"BitNet Integration"})}),"\n",(0,r.jsx)(t.h2,{id:"what-is-bitnet-b158",children:"What is BitNet b1.58?"}),"\n",(0,r.jsxs)(t.p,{children:["BitNet b1.58 is a neural network architecture developed by Microsoft Research that constrains all weight parameters to ternary values: ",(0,r.jsx)(t.strong,{children:"{-1, 0, +1}"}),'. The name "b1.58" refers to the information-theoretic density of each parameter -- since log2(3) is approximately 1.58, each ternary weight encodes 1.58 bits of information. This stands in contrast to conventional neural networks that use 32-bit floating point (float32) or 16-bit floating point (float16) values for their weights.']}),"\n",(0,r.jsxs)(t.p,{children:["The implications of this constraint are significant. A model with ternary weights requires roughly ",(0,r.jsx)(t.strong,{children:"20x less memory"})," than its float32 equivalent, because each parameter needs only 1.58 bits instead of 32 bits. A 2-billion-parameter model that would normally consume over 7 GB in float32 can be stored in approximately 400 MB with ternary quantization. Beyond memory savings, ternary weights transform the core matrix-vector multiply operation from expensive floating-point multiplications into simple additions and subtractions. When a weight is +1, you add the activation; when it is -1, you subtract it; when it is 0, you skip it entirely. No multiplication hardware is required."]}),"\n",(0,r.jsx)(t.h2,{id:"why-trinity-implements-bitnet-natively",children:"Why Trinity Implements BitNet Natively"}),"\n",(0,r.jsx)(t.p,{children:"Trinity is a ternary computing framework, and its core data representation -- the trit with values {-1, 0, +1} -- maps directly onto BitNet's weight space. This is not a coincidence; ternary arithmetic is mathematically optimal for information density and computational efficiency. Trinity's Vector Symbolic Architecture (VSA) operations, its packed trit encoding, and its ternary virtual machine all operate in the same {-1, 0, +1} space that BitNet demands."}),"\n",(0,r.jsx)(t.p,{children:"Rather than relying on external inference runtimes like llama.cpp or PyTorch, Trinity implements the entire BitNet inference pipeline natively in Zig. This means zero external dependencies, direct control over memory layout, and the ability to leverage SIMD-optimized ternary matrix-vector operations that exploit the add-only nature of ternary computation."}),"\n",(0,r.jsx)(t.h2,{id:"inference-pipeline-overview",children:"Inference Pipeline Overview"}),"\n",(0,r.jsx)(t.p,{children:"The Trinity BitNet inference pipeline consists of four major stages:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"GGUF Model Loading"})," -- Trinity includes a purpose-built GGUF v3 format reader (",(0,r.jsx)(t.code,{children:"gguf_reader.zig"}),") that parses model files from the llama.cpp ecosystem. It supports BitNet-specific quantization types including I2_S (2-bit integer with scale), TQ1_0 (pure ternary packed), and TQ2_0 (ternary with scale). The reader extracts model architecture metadata, tensor layouts, and weight data."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"SentencePiece Tokenization"})," -- Text input is tokenized using a SentencePiece BPE tokenizer (",(0,r.jsx)(t.code,{children:"sentencepiece_tokenizer.zig"}),") that supports both the SentencePiece space marker convention and the GPT-2/Llama 3 convention. The tokenizer handles a vocabulary of up to 128,256 tokens, with proper byte fallback encoding for out-of-vocabulary characters."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"30-Layer Transformer"})," -- The core inference engine (",(0,r.jsx)(t.code,{children:"bitnet_full_layers.zig"}),") implements a complete 30-layer transformer with the following configuration: hidden size of 2560, 20 attention heads with 5 key-value heads (Grouped Query Attention with a 4:1 ratio), intermediate feed-forward size of 6912, and a maximum context length of 4096 positions. Each layer performs RMSNorm, multi-head attention with KV-cache, and a feed-forward network with gate/up/down projections using the squared ReLU (relu2) activation function. Rotary Position Embeddings (RoPE) with theta=500000 encode positional information."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Text Generation"})," -- After the final transformer layer, a final RMSNorm is applied, logits are computed over the vocabulary, and temperature-based sampling selects the next token. The decoded token is appended to the output and fed back for autoregressive generation."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"research-foundation",children:"Research Foundation"}),"\n",(0,r.jsx)(t.p,{children:'BitNet b1.58 was introduced by Microsoft Research in the paper "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits" (Ma et al., 2024). The research demonstrated that ternary-weight transformers can match the performance of full-precision models at equivalent parameter counts, while achieving dramatically lower memory footprint and computational cost. Trinity builds on this research by providing a native ternary inference runtime that fully exploits the mathematical properties of {-1, 0, +1} arithmetic.'})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,t,i){i.d(t,{R:()=>o,x:()=>s});var n=i(6540);const r={},a=n.createContext(r);function o(e){const t=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(a.Provider,{value:t},e.children)}}}]);