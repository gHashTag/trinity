"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[3279],{8903(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"deployment/local","title":"Local Deployment","description":"Run Trinity on your local machine for development, testing, and inference with ternary models. This guide covers building from source, running inference, and using the CLI tools.","source":"@site/docs/deployment/local.md","sourceDirName":"deployment","slug":"/deployment/local","permalink":"/trinity/docs/deployment/local","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/deployment/local.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docsSidebar","previous":{"title":"RunPod GPU Deployment","permalink":"/trinity/docs/deployment/runpod"},"next":{"title":"Overview","permalink":"/trinity/docs/api/"}}');var l=i(4848),s=i(8453);const t={sidebar_position:3},d="Local Deployment",o={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Build from Source",id:"build-from-source",level:2},{value:"macOS",id:"macos",level:3},{value:"Linux",id:"linux",level:3},{value:"Windows",id:"windows",level:3},{value:"Verify the Build",id:"verify-the-build",level:3},{value:"Running Inference with Local Models",id:"running-inference-with-local-models",level:2},{value:"Obtaining GGUF Models",id:"obtaining-gguf-models",level:3},{value:"Chat Mode",id:"chat-mode",level:3},{value:"Server Mode",id:"server-mode",level:3},{value:"Memory Requirements by Model Size",id:"memory-requirements-by-model-size",level:2},{value:"CPU Performance Expectations",id:"cpu-performance-expectations",level:2},{value:"Other CLI Commands",id:"other-cli-commands",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"local-deployment",children:"Local Deployment"})}),"\n",(0,l.jsx)(n.p,{children:"Run Trinity on your local machine for development, testing, and inference with ternary models. This guide covers building from source, running inference, and using the CLI tools."}),"\n",(0,l.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Requirement"}),(0,l.jsx)(n.th,{children:"Version"}),(0,l.jsx)(n.th,{children:"Notes"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Zig"}),(0,l.jsx)(n.td,{children:"0.13.0"}),(0,l.jsx)(n.td,{children:"Exact version required"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Git"}),(0,l.jsx)(n.td,{children:"Any recent"}),(0,l.jsx)(n.td,{children:"For cloning the repository"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"RAM"}),(0,l.jsx)(n.td,{children:"4 GB minimum"}),(0,l.jsx)(n.td,{children:"8 GB+ recommended for model inference"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Disk"}),(0,l.jsx)(n.td,{children:"1 GB minimum"}),(0,l.jsx)(n.td,{children:"Plus model file size"})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"build-from-source",children:"Build from Source"}),"\n",(0,l.jsx)(n.h3,{id:"macos",children:"macOS"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'# Install Zig (Apple Silicon)\ncurl -LO https://ziglang.org/download/0.13.0/zig-macos-aarch64-0.13.0.tar.xz\ntar -xf zig-macos-aarch64-0.13.0.tar.xz\nexport PATH="$PWD/zig-macos-aarch64-0.13.0:$PATH"\n\n# Alternatively, use Homebrew\nbrew install zig@0.13\n\n# Clone and build\ngit clone https://github.com/gHashTag/trinity.git\ncd trinity\nzig build\n'})}),"\n",(0,l.jsx)(n.h3,{id:"linux",children:"Linux"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'# Install Zig\ncurl -LO https://ziglang.org/download/0.13.0/zig-linux-x86_64-0.13.0.tar.xz\ntar -xf zig-linux-x86_64-0.13.0.tar.xz\nexport PATH="$PWD/zig-linux-x86_64-0.13.0:$PATH"\n\n# Clone and build\ngit clone https://github.com/gHashTag/trinity.git\ncd trinity\nzig build\n'})}),"\n",(0,l.jsx)(n.h3,{id:"windows",children:"Windows"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Download Zig 0.13.0 from ",(0,l.jsx)(n.a,{href:"https://ziglang.org/download/",children:"ziglang.org/download"})]}),"\n",(0,l.jsxs)(n.li,{children:["Extract to ",(0,l.jsx)(n.code,{children:"C:\\zig"})," and add to your PATH"]}),"\n",(0,l.jsx)(n.li,{children:"Clone and build:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-powershell",children:"git clone https://github.com/gHashTag/trinity.git\ncd trinity\nzig build\n"})}),"\n",(0,l.jsx)(n.h3,{id:"verify-the-build",children:"Verify the Build"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"zig build test\n"})}),"\n",(0,l.jsx)(n.p,{children:"All tests should pass. You can also run specific module tests:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"zig test src/vsa.zig     # VSA operations\nzig test src/vm.zig      # Virtual machine\n"})}),"\n",(0,l.jsx)(n.h2,{id:"running-inference-with-local-models",children:"Running Inference with Local Models"}),"\n",(0,l.jsx)(n.h3,{id:"obtaining-gguf-models",children:"Obtaining GGUF Models"}),"\n",(0,l.jsx)(n.p,{children:"BitNet b1.58 models in GGUF format are available from HuggingFace:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"microsoft/bitnet-b1.58-2B-4T-gguf"})," -- 2.4B parameter model, ~1.1 GB"]}),"\n",(0,l.jsx)(n.li,{children:"Other ternary models can be converted to GGUF using the tools provided with bitnet.cpp"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Download using the HuggingFace CLI or directly:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"pip install huggingface_hub\npython -c \"\nfrom huggingface_hub import hf_hub_download\nhf_hub_download('microsoft/bitnet-b1.58-2B-4T-gguf', 'ggml-model-i2_s.gguf', local_dir='./models')\n\"\n"})}),"\n",(0,l.jsx)(n.h3,{id:"chat-mode",children:"Chat Mode"}),"\n",(0,l.jsx)(n.p,{children:"Start an interactive chat session with a local model:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"./bin/vibee chat --model ./models/ggml-model-i2_s.gguf\n"})}),"\n",(0,l.jsx)(n.h3,{id:"server-mode",children:"Server Mode"}),"\n",(0,l.jsx)(n.p,{children:"Run Trinity as an HTTP server for API-based inference:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"./bin/vibee serve --port 8080\n"})}),"\n",(0,l.jsx)(n.p,{children:"This starts a local HTTP server that accepts inference requests via JSON API."}),"\n",(0,l.jsx)(n.h2,{id:"memory-requirements-by-model-size",children:"Memory Requirements by Model Size"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Model"}),(0,l.jsx)(n.th,{children:"Parameters"}),(0,l.jsx)(n.th,{children:"GGUF File Size"}),(0,l.jsx)(n.th,{children:"Min RAM (inference)"}),(0,l.jsx)(n.th,{children:"Recommended RAM"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"BitNet Small"}),(0,l.jsx)(n.td,{children:"~700M"}),(0,l.jsx)(n.td,{children:"~350 MB"}),(0,l.jsx)(n.td,{children:"2 GB"}),(0,l.jsx)(n.td,{children:"4 GB"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"BitNet 2B-4T"}),(0,l.jsx)(n.td,{children:"2.4B"}),(0,l.jsx)(n.td,{children:"1.1 GB"}),(0,l.jsx)(n.td,{children:"4 GB"}),(0,l.jsx)(n.td,{children:"8 GB"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"BitNet 3B"}),(0,l.jsx)(n.td,{children:"~3B"}),(0,l.jsx)(n.td,{children:"~1.4 GB"}),(0,l.jsx)(n.td,{children:"4 GB"}),(0,l.jsx)(n.td,{children:"8 GB"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"BitNet 7B"}),(0,l.jsx)(n.td,{children:"~7B"}),(0,l.jsx)(n.td,{children:"~3.2 GB"}),(0,l.jsx)(n.td,{children:"8 GB"}),(0,l.jsx)(n.td,{children:"16 GB"})]})]})]}),"\n",(0,l.jsx)(n.p,{children:"These numbers reflect the ternary-packed model weights. During inference, additional memory is required for the KV cache (which scales with context length) and activation buffers."}),"\n",(0,l.jsx)(n.h2,{id:"cpu-performance-expectations",children:"CPU Performance Expectations"}),"\n",(0,l.jsx)(n.p,{children:"Local CPU inference is significantly slower than GPU inference. On an Apple M1 Pro or comparable x86 CPU, expect:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Without optimized kernels"}),": 0.1-0.5 tokens/second (very slow)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"With AVX-512 VNNI (x86)"}),": Up to ~15,000 tokens/second"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"ARM NEON (Apple Silicon)"}),": Performance depends on kernel availability"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["For production-grade throughput, see the ",(0,l.jsx)(n.a,{href:"/docs/deployment/runpod",children:"RunPod GPU Deployment"})," guide."]}),"\n",(0,l.jsx)(n.h2,{id:"other-cli-commands",children:"Other CLI Commands"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# Generate code from a .vibee specification\n./bin/vibee gen specs/tri/module.vibee\n\n# Run a program via the bytecode VM\n./bin/vibee run program.999\n\n# Build the Firebird LLM CLI in release mode\nzig build firebird\n\n# Cross-platform release builds\nzig build release\n"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(a,{...e})}):a(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>d});var r=i(6540);const l={},s=r.createContext(l);function t(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);