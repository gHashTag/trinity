"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[5387],{9616(e,r,i){i.r(r),i.d(r,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"benchmarks/index","title":"Benchmarks","description":"Trinity is a high-performance ternary computing framework built in Zig, designed for both Vector Symbolic Architecture (VSA) operations and large language model inference using BitNet b1.58 ternary weights. This section provides an overview of Trinity\'s performance characteristics across several key dimensions.","source":"@site/docs/benchmarks/index.md","sourceDirName":"benchmarks","slug":"/benchmarks/","permalink":"/trinity/docs/benchmarks/","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/benchmarks/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Theorems","permalink":"/trinity/docs/vibee/theorems"},"next":{"title":"GPU Inference Benchmarks","permalink":"/trinity/docs/benchmarks/gpu-inference"}}');var t=i(4848),s=i(8453);const o={sidebar_position:1},a="Benchmarks",c={},d=[{value:"Key Performance Metrics",id:"key-performance-metrics",level:2},{value:"Additional Results",id:"additional-results",level:3},{value:"Why Ternary is Fast",id:"why-ternary-is-fast",level:2},{value:"Performance Areas",id:"performance-areas",level:2},{value:"GPU Inference",id:"gpu-inference",level:3},{value:"JIT Compilation",id:"jit-compilation",level:3},{value:"Memory Efficiency",id:"memory-efficiency",level:3},{value:"Competitor Comparison",id:"competitor-comparison",level:3},{value:"Ternary Arithmetic Advantage",id:"ternary-arithmetic-advantage",level:2}];function l(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"benchmarks",children:"Benchmarks"})}),"\n",(0,t.jsx)(r.p,{children:"Trinity is a high-performance ternary computing framework built in Zig, designed for both Vector Symbolic Architecture (VSA) operations and large language model inference using BitNet b1.58 ternary weights. This section provides an overview of Trinity's performance characteristics across several key dimensions."}),"\n",(0,t.jsx)(r.h2,{id:"key-performance-metrics",children:"Key Performance Metrics"}),"\n",(0,t.jsxs)(r.table,{children:[(0,t.jsx)(r.thead,{children:(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.th,{children:"Metric"}),(0,t.jsx)(r.th,{children:"Value"}),(0,t.jsx)(r.th,{children:"Notes"})]})}),(0,t.jsxs)(r.tbody,{children:[(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"GPU inference throughput"}),(0,t.jsx)(r.td,{children:"Up to 298K tokens/sec"}),(0,t.jsx)(r.td,{children:"RTX 3090, BitNet b1.58 via bitnet.cpp"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"JIT speedup"}),(0,t.jsx)(r.td,{children:"15-260x"}),(0,t.jsx)(r.td,{children:"Over interpreted Zig execution for VSA ops"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Memory compression"}),(0,t.jsx)(r.td,{children:"20x"}),(0,t.jsx)(r.td,{children:"Ternary packed vs float32 representation"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Compute model"}),(0,t.jsx)(r.td,{children:"Add-only"}),(0,t.jsx)(r.td,{children:"No multiply operations required for ternary weights"})]})]})]}),"\n",(0,t.jsx)(r.h3,{id:"additional-results",children:"Additional Results"}),"\n",(0,t.jsxs)(r.table,{children:[(0,t.jsx)(r.thead,{children:(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.th,{children:"Metric"}),(0,t.jsx)(r.th,{children:"Value"}),(0,t.jsx)(r.th,{children:"Notes"})]})}),(0,t.jsxs)(r.tbody,{children:[(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"SIMD ternary matmul"}),(0,t.jsx)(r.td,{children:"7.65 GFLOPS"}),(0,t.jsx)(r.td,{children:"BatchTiled, 2.28x over SIMD-16 baseline"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Model load time"}),(0,t.jsx)(r.td,{children:"4.8s (NVMe)"}),(0,t.jsx)(r.td,{children:"43x improvement over 208s (ephemeral disk)"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"HDC continual learning"}),(0,t.jsx)(r.td,{children:"3% avg forgetting"}),(0,t.jsx)(r.td,{children:"20 classes, 10 phases (vs 50-90% neural nets)"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"BitNet coherent text"}),(0,t.jsx)(r.td,{children:"Confirmed"}),(0,t.jsx)(r.td,{children:"bitnet.cpp on RunPod RTX 4090"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Unit tests passing"}),(0,t.jsx)(r.td,{children:"143"}),(0,t.jsx)(r.td,{children:"Across all subsystems"})]})]})]}),"\n",(0,t.jsx)(r.h2,{id:"why-ternary-is-fast",children:"Why Ternary is Fast"}),"\n",(0,t.jsxs)(r.p,{children:["Ternary {-1, 0, +1} weights eliminate the need for multiplication in matrix-vector products. Instead of ",(0,t.jsx)(r.code,{children:"weight * activation"}),", the operation reduces to addition, subtraction, or skip. This has two major consequences: dramatically lower memory bandwidth requirements (1.58 bits per weight vs 32 bits for float32) and simpler arithmetic that maps efficiently to both CPU SIMD instructions and custom hardware."]}),"\n",(0,t.jsx)(r.h2,{id:"performance-areas",children:"Performance Areas"}),"\n",(0,t.jsx)(r.h3,{id:"gpu-inference",children:"GPU Inference"}),"\n",(0,t.jsxs)(r.p,{children:["BitNet b1.58 models running on consumer and datacenter GPUs achieve throughput measured in hundreds of thousands of tokens per second for small models. Performance varies by GPU type, model size, and batch configuration. See ",(0,t.jsx)(r.a,{href:"/docs/benchmarks/gpu-inference",children:"GPU Inference Benchmarks"})," for detailed numbers."]}),"\n",(0,t.jsx)(r.h3,{id:"jit-compilation",children:"JIT Compilation"}),"\n",(0,t.jsxs)(r.p,{children:["Trinity includes a custom JIT compiler with backends for ARM64 (Apple Silicon, Raspberry Pi, etc.) and x86-64 (Intel/AMD). VSA operations such as bind, bundle, dot product, and permute are compiled to native machine code at runtime, with compiled functions cached for reuse. See ",(0,t.jsx)(r.a,{href:"/docs/benchmarks/jit-performance",children:"JIT Compilation Performance"})," for architecture-specific results."]}),"\n",(0,t.jsx)(r.h3,{id:"memory-efficiency",children:"Memory Efficiency"}),"\n",(0,t.jsxs)(r.p,{children:["The framework provides multiple memory representations optimized for different use cases: HybridBigInt with lazy packed/unpacked conversion, bit-packed trit arrays, and sparse COO-format vectors for data with many zeros. A 10,000-dimensional vector that would consume 40KB in float32 fits in roughly 2.5KB using packed ternary encoding. See ",(0,t.jsx)(r.a,{href:"/docs/benchmarks/memory-efficiency",children:"Memory Efficiency"})," for a detailed breakdown."]}),"\n",(0,t.jsx)(r.h3,{id:"competitor-comparison",children:"Competitor Comparison"}),"\n",(0,t.jsxs)(r.p,{children:["How does Trinity stack up against Groq, GPT-4, and other LLM providers? Trinity offers 35-52 tok/s on CPU with self-hosted costs of $0.01-0.35/hr, compared to cloud providers charging per-token fees. See ",(0,t.jsx)(r.a,{href:"/docs/benchmarks/competitor-comparison",children:"Competitor Comparison"})," for detailed benchmarks and cost analysis."]}),"\n",(0,t.jsx)(r.h2,{id:"ternary-arithmetic-advantage",children:"Ternary Arithmetic Advantage"}),"\n",(0,t.jsx)(r.p,{children:"The mathematical basis for ternary efficiency comes from information theory. The optimal radix for information density is Euler's number (e ~ 2.718), and 3 is the closest integer. Each trit carries 1.58 bits of information (log2(3)), compared to 1 bit per binary digit. This means ternary representations achieve higher information density per storage unit, which translates directly to reduced memory footprint and bandwidth consumption in real workloads."})]})}function h(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453(e,r,i){i.d(r,{R:()=>o,x:()=>a});var n=i(6540);const t={},s=n.createContext(t);function o(e){const r=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),n.createElement(s.Provider,{value:r},e.children)}}}]);