"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[625],{7737(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"api/sequence-hdc","title":"Sequence HDC API","description":"This module turns text into vectors. Feed it strings like \\"hello world\\", and it produces compact numeric vectors that capture the text\'s pattern. Similar texts produce similar vectors. Use it for language detection, text classification, or semantic search -- without training a neural network.","source":"@site/docs/api/sequence-hdc.md","sourceDirName":"api","slug":"/api/sequence-hdc","permalink":"/trinity/docs/api/sequence-hdc","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/api/sequence-hdc.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"docsSidebar","previous":{"title":"Plugin API","permalink":"/trinity/docs/api/plugin"},"next":{"title":"JIT Compilation API","permalink":"/trinity/docs/api/jit"}}');var i=r(4848),s=r(8453);const o={sidebar_position:8},c="Sequence HDC API",l={},d=[{value:"Pipeline Overview",id:"pipeline-overview",level:2},{value:"Step-by-Step Walkthrough",id:"step-by-step-walkthrough",level:2},{value:"Interpreting Similarity Scores",id:"interpreting-similarity-scores",level:2},{value:"Parameter Selection Guide",id:"parameter-selection-guide",level:2},{value:"Dimension",id:"dimension",level:3},{value:"N-gram Size",id:"n-gram-size",level:3},{value:"ItemMemory",id:"itemmemory",level:2},{value:"Construction",id:"construction",level:3},{value:"Methods",id:"methods",level:3},{value:"<code>init(allocator: Allocator, dimension: usize, seed: u64) ItemMemory</code>",id:"initallocator-allocator-dimension-usize-seed-u64-itemmemory",level:4},{value:"<code>deinit(self: *ItemMemory) void</code>",id:"deinitself-itemmemory-void",level:4},{value:"<code>getVector(self: *ItemMemory, symbol: u32) !*HybridBigInt</code>",id:"getvectorself-itemmemory-symbol-u32-hybridbigint",level:4},{value:"<code>getCharVector(self: *ItemMemory, char: u8) !*HybridBigInt</code>",id:"getcharvectorself-itemmemory-char-u8-hybridbigint",level:4},{value:"<code>encodeString(self: *ItemMemory, str: []const u8) ![]HybridBigInt</code>",id:"encodestringself-itemmemory-str-const-u8-hybridbigint",level:4},{value:"Properties",id:"properties",level:3},{value:"NGramEncoder",id:"ngramencoder",level:2},{value:"Construction",id:"construction-1",level:3},{value:"Methods",id:"methods-1",level:3},{value:"<code>init(item_memory: *ItemMemory, n: usize) NGramEncoder</code>",id:"inititem_memory-itemmemory-n-usize-ngramencoder",level:4},{value:"<code>enableJIT(self: *NGramEncoder, engine: *JitVSAEngine) void</code>",id:"enablejitself-ngramencoder-engine-jitvsaengine-void",level:4},{value:"<code>encodeNGram(self: *NGramEncoder, chars: []const u8) !HybridBigInt</code>",id:"encodengramself-ngramencoder-chars-const-u8-hybridbigint",level:4},{value:"<code>encodeAllNGrams(self: *NGramEncoder, allocator: Allocator, str: []const u8) ![]HybridBigInt</code>",id:"encodeallngramsself-ngramencoder-allocator-allocator-str-const-u8-hybridbigint",level:4},{value:"SequenceMemory",id:"sequencememory",level:2},{value:"Encoding Pipeline",id:"encoding-pipeline",level:3},{value:"Construction",id:"construction-2",level:3},{value:"Methods",id:"methods-2",level:3},{value:"<code>init(allocator: Allocator, dimension: usize, n: usize, seed: u64) SequenceMemory</code>",id:"initallocator-allocator-dimension-usize-n-usize-seed-u64-sequencememory",level:4},{value:"<code>deinit(self: *SequenceMemory) void</code>",id:"deinitself-sequencememory-void",level:4},{value:"<code>enableJIT(self: *SequenceMemory) void</code>",id:"enablejitself-sequencememory-void",level:4},{value:"<code>encode(self: *SequenceMemory, str: []const u8) !HybridBigInt</code>",id:"encodeself-sequencememory-str-const-u8-hybridbigint",level:4},{value:"<code>store(self: *SequenceMemory, label: []const u8, str: []const u8) !void</code>",id:"storeself-sequencememory-label-const-u8-str-const-u8-void",level:4},{value:"<code>query(self: *SequenceMemory, str: []const u8) !?QueryResult</code>",id:"queryself-sequencememory-str-const-u8-queryresult",level:4},{value:"<code>queryTopK(self: *SequenceMemory, str: []const u8, k: usize) ![]QueryResult</code>",id:"querytopkself-sequencememory-str-const-u8-k-usize-queryresult",level:4},{value:"QueryResult",id:"queryresult",level:3},{value:"LanguageDetector",id:"languagedetector",level:2},{value:"Construction",id:"construction-3",level:3},{value:"Methods",id:"methods-3",level:3},{value:"<code>init(allocator: Allocator, dimension: usize, n: usize, seed: u64) LanguageDetector</code>",id:"initallocator-allocator-dimension-usize-n-usize-seed-u64-languagedetector",level:4},{value:"<code>enableJIT(self: *LanguageDetector) void</code>",id:"enablejitself-languagedetector-void",level:4},{value:"<code>deinit(self: *LanguageDetector) void</code>",id:"deinitself-languagedetector-void",level:4},{value:"<code>train(self: *LanguageDetector, language: []const u8, sample: []const u8) !void</code>",id:"trainself-languagedetector-language-const-u8-sample-const-u8-void",level:4},{value:"<code>detect(self: *LanguageDetector, text: []const u8) !?QueryResult</code>",id:"detectself-languagedetector-text-const-u8-queryresult",level:4},{value:"Complete Example: Language Detection",id:"complete-example-language-detection",level:2}];function a(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sequence-hdc-api",children:"Sequence HDC API"})}),"\n",(0,i.jsx)(n.p,{children:'This module turns text into vectors. Feed it strings like "hello world", and it produces compact numeric vectors that capture the text\'s pattern. Similar texts produce similar vectors. Use it for language detection, text classification, or semantic search -- without training a neural network.'}),"\n",(0,i.jsxs)(n.p,{children:["Under the hood, the module uses ",(0,i.jsx)(n.a,{href:"/docs/concepts/glossary",children:"Hyperdimensional Computing"})," (HDC). It maps characters to high-dimensional ",(0,i.jsx)(n.a,{href:"/docs/concepts/glossary",children:"ternary vectors"})," ({-1, 0, +1}), then combines them to represent words, phrases, and documents. The key insight: texts that share character patterns produce vectors that point in similar directions."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Source:"})," ",(0,i.jsx)(n.code,{children:"src/sequence_hdc.zig"})]}),"\n",(0,i.jsx)(n.h2,{id:"pipeline-overview",children:"Pipeline Overview"}),"\n",(0,i.jsx)(n.p,{children:"Every text goes through the same four-stage pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph LR\n  A["Input Text"] --\x3e B["Split into<br/>n-grams"]\n  B --\x3e C["Encode each<br/>n-gram"]\n  C --\x3e D["Bundle all<br/>into one vector"]\n  D --\x3e E["Compare via<br/>cosine similarity"]\n'})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Split"})," the input into overlapping character ",(0,i.jsx)(n.a,{href:"/docs/concepts/glossary",children:"n-grams"})," (e.g., trigrams)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Encode"})," each n-gram by looking up character vectors and combining them."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Bundle"})," all n-gram vectors into a single vector using majority vote."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compare"})," the result to stored vectors using cosine similarity."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-by-step-walkthrough",children:"Step-by-Step Walkthrough"}),"\n",(0,i.jsxs)(n.p,{children:["Let's trace how the word ",(0,i.jsx)(n.strong,{children:'"cat"'})," becomes a vector."]}),"\n",(0,i.jsxs)(n.p,{children:['We use trigrams (n=3), so "cat" produces one trigram: ',(0,i.jsx)(n.code,{children:"c"}),", ",(0,i.jsx)(n.code,{children:"a"}),", ",(0,i.jsx)(n.code,{children:"t"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1 -- Look up character vectors."})," Each character maps to a random high-dimensional vector from the ",(0,i.jsx)(n.a,{href:"#itemmemory",children:"ItemMemory"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"c -> [+1, -1, 0, +1, -1, 0, ...]   (1000 random trits)\na -> [ 0, +1, -1, +1, 0, -1, ...]\nt -> [-1, 0, +1, 0, +1, +1, ...]\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2 -- Permute by position."}),' Each character vector shifts by its position index within the trigram. This encodes order -- "cat" and "act" produce different results.']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"perm(c, 2)  -- shift c's vector right by 2\nperm(a, 1)  -- shift a's vector right by 1\nperm(t, 0)  -- no shift (position 0)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3 -- Bind them together."})," Element-wise ternary multiplication fuses the three permuted vectors into one:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"trigram_vector = bind(perm(c, 2), bind(perm(a, 1), perm(t, 0)))\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4 -- Bundle (for longer text)."}),' For "cat" there is only one trigram, so the trigram vector ',(0,i.jsx)(n.em,{children:"is"}),' the final vector. For "cats", you would get trigrams "cat" and "ats", and their vectors would be bundled via majority vote.']}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["The encoding is ",(0,i.jsx)(n.strong,{children:"order-sensitive"}),'. "cat" and "tac" produce different vectors because permutation shifts differ by position. This is essential for distinguishing real text patterns.']})}),"\n",(0,i.jsx)(n.h2,{id:"interpreting-similarity-scores",children:"Interpreting Similarity Scores"}),"\n",(0,i.jsx)(n.p,{children:"After encoding, you compare vectors with cosine similarity. Here is a rough guide for interpreting scores:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Score Range"}),(0,i.jsx)(n.th,{children:"Interpretation"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"0.3 and above"}),(0,i.jsx)(n.td,{children:"Confident match -- texts share strong character patterns"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"0.1 to 0.3"}),(0,i.jsx)(n.td,{children:"Possible match -- some shared structure"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Near 0"}),(0,i.jsx)(n.td,{children:"Unrelated -- no meaningful overlap"})]})]})]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"These thresholds are approximate guidelines, not hard rules. Your specific use case may need different cutoffs. Train on representative data and calibrate accordingly."})}),"\n",(0,i.jsx)(n.h2,{id:"parameter-selection-guide",children:"Parameter Selection Guide"}),"\n",(0,i.jsxs)(n.p,{children:["Two parameters dominate behavior: ",(0,i.jsx)(n.strong,{children:"dimension"})," and ",(0,i.jsx)(n.strong,{children:"n-gram size"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"dimension",children:"Dimension"}),"\n",(0,i.jsx)(n.p,{children:"Higher dimensions give the vector space more room to separate different patterns. Lower dimensions are faster but risk collisions."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Use Case"}),(0,i.jsx)(n.th,{children:"Recommended Dimension"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Quick tests"}),(0,i.jsx)(n.td,{children:"256--1000"}),(0,i.jsx)(n.td,{children:"Fast but limited capacity"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Text classification"}),(0,i.jsx)(n.td,{children:"2000--4000"}),(0,i.jsx)(n.td,{children:"Good balance of speed and accuracy"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Large vocabularies"}),(0,i.jsx)(n.td,{children:"8000--10000"}),(0,i.jsx)(n.td,{children:"High capacity, slower without JIT"})]})]})]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"4000 dimensions"})," is a strong default for most text tasks. It gives enough capacity for multi-language detection while staying fast enough for interactive use."]})}),"\n",(0,i.jsx)(n.h3,{id:"n-gram-size",children:"N-gram Size"}),"\n",(0,i.jsx)(n.p,{children:"The n-gram size controls how much local context each encoding captures."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"n=2 (bigrams):"})," Fast, but too coarse for most natural language tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"n=3 (trigrams):"})," Best default. Captures character patterns that distinguish languages and word families."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"n=4+:"})," More context, but increases computation and can overfit on short texts."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"itemmemory",children:"ItemMemory"}),"\n",(0,i.jsxs)(n.p,{children:["Maps symbol IDs (or ASCII characters) to deterministically generated random ",(0,i.jsx)(n.a,{href:"/docs/concepts/glossary",children:"hypervectors"}),". Vectors are lazily created on first access and cached in a ",(0,i.jsx)(n.code,{children:"HashMap"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Each trit in a generated vector is uniformly random from {-1, 0, +1}, seeded by ",(0,i.jsx)(n.code,{children:"symbol_id * 2654435761 + seed"})," using the standard PRNG."]}),"\n",(0,i.jsx)(n.h3,{id:"construction",children:"Construction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const ItemMemory = @import("sequence_hdc").ItemMemory;\n\n// Create item memory with dimension=1000, seed=42\nvar item_mem = ItemMemory.init(allocator, 1000, 42);\ndefer item_mem.deinit();\n'})}),"\n",(0,i.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,i.jsx)(n.h4,{id:"initallocator-allocator-dimension-usize-seed-u64-itemmemory",children:(0,i.jsx)(n.code,{children:"init(allocator: Allocator, dimension: usize, seed: u64) ItemMemory"})}),"\n",(0,i.jsx)(n.p,{children:"Creates a new item memory with the specified vector dimension and random seed. The seed ensures reproducibility: the same seed always generates the same vectors for each symbol."}),"\n",(0,i.jsx)(n.h4,{id:"deinitself-itemmemory-void",children:(0,i.jsx)(n.code,{children:"deinit(self: *ItemMemory) void"})}),"\n",(0,i.jsx)(n.p,{children:"Frees the internal cache. Call when the item memory is no longer needed."}),"\n",(0,i.jsx)(n.h4,{id:"getvectorself-itemmemory-symbol-u32-hybridbigint",children:(0,i.jsx)(n.code,{children:"getVector(self: *ItemMemory, symbol: u32) !*HybridBigInt"})}),"\n",(0,i.jsx)(n.p,{children:"Returns a pointer to the hypervector for the given symbol ID. If the vector does not exist, it is deterministically generated from the symbol ID and seed, then cached. Subsequent calls with the same symbol return the cached vector."}),"\n",(0,i.jsx)(n.h4,{id:"getcharvectorself-itemmemory-char-u8-hybridbigint",children:(0,i.jsx)(n.code,{children:"getCharVector(self: *ItemMemory, char: u8) !*HybridBigInt"})}),"\n",(0,i.jsxs)(n.p,{children:["Convenience wrapper that calls ",(0,i.jsx)(n.code,{children:"getVector"})," with the character cast to ",(0,i.jsx)(n.code,{children:"u32"}),". Maps ASCII characters to hypervectors."]}),"\n",(0,i.jsx)(n.h4,{id:"encodestringself-itemmemory-str-const-u8-hybridbigint",children:(0,i.jsx)(n.code,{children:"encodeString(self: *ItemMemory, str: []const u8) ![]HybridBigInt"})}),"\n",(0,i.jsxs)(n.p,{children:["Encodes an entire string as an array of character hypervectors. Returns a newly allocated slice with one ",(0,i.jsx)(n.code,{children:"HybridBigInt"})," per character. The caller owns the returned memory."]}),"\n",(0,i.jsx)(n.h3,{id:"properties",children:"Properties"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deterministic:"})," Same ",(0,i.jsx)(n.code,{children:"(symbol, seed)"})," pair always produces the same vector."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quasi-orthogonal:"})," Random high-dimensional vectors are nearly orthogonal (cosine similarity close to 0)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lazy caching:"})," Vectors are generated on first access and reused thereafter."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"ngramencoder",children:"NGramEncoder"}),"\n",(0,i.jsxs)(n.p,{children:["Encodes character ",(0,i.jsx)(n.a,{href:"/docs/concepts/glossary",children:"n-grams"}),' using position-encoded binding. Each character in an n-gram shifts by its position index, then all characters bind together. This preserves order: "abc" and "bac" produce different vectors.']}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsx)("summary",{children:"Encoding Formula"}),(0,i.jsxs)(n.p,{children:["For an n-gram of characters ",(0,i.jsx)(n.code,{children:"c[0], c[1], ..., c[n-1]"}),":"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ngram = bind(perm(c[0], n-1), bind(perm(c[1], n-2), ..., perm(c[n-1], 0)))\n"})}),(0,i.jsxs)(n.p,{children:["Where ",(0,i.jsx)(n.code,{children:"perm(v, k)"})," is cyclic permutation by ",(0,i.jsx)(n.code,{children:"k"})," positions and ",(0,i.jsx)(n.code,{children:"bind(a, b)"})," is element-wise ternary multiplication."]})]}),"\n",(0,i.jsx)(n.h3,{id:"construction-1",children:"Construction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const NGramEncoder = @import("sequence_hdc").NGramEncoder;\n\nvar encoder = NGramEncoder.init(&item_mem, 3); // trigram encoder\n'})}),"\n",(0,i.jsx)(n.h3,{id:"methods-1",children:"Methods"}),"\n",(0,i.jsx)(n.h4,{id:"inititem_memory-itemmemory-n-usize-ngramencoder",children:(0,i.jsx)(n.code,{children:"init(item_memory: *ItemMemory, n: usize) NGramEncoder"})}),"\n",(0,i.jsxs)(n.p,{children:["Creates an n-gram encoder that pulls character vectors from the given item memory. The parameter ",(0,i.jsx)(n.code,{children:"n"})," sets the n-gram size (e.g., 3 for trigrams)."]}),"\n",(0,i.jsx)(n.h4,{id:"enablejitself-ngramencoder-engine-jitvsaengine-void",children:(0,i.jsx)(n.code,{children:"enableJIT(self: *NGramEncoder, engine: *JitVSAEngine) void"})}),"\n",(0,i.jsx)(n.p,{children:"Enables JIT acceleration for bind operations within n-gram encoding. The engine must outlive the encoder."}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"Enable JIT when encoding many strings or working with high dimensions. For a single short string, the compilation overhead outweighs the speedup."})}),"\n",(0,i.jsx)(n.h4,{id:"encodengramself-ngramencoder-chars-const-u8-hybridbigint",children:(0,i.jsx)(n.code,{children:"encodeNGram(self: *NGramEncoder, chars: []const u8) !HybridBigInt"})}),"\n",(0,i.jsxs)(n.p,{children:["Encodes a single n-gram from a character slice. The slice length should equal or exceed ",(0,i.jsx)(n.code,{children:"n"}),"; only the first ",(0,i.jsx)(n.code,{children:"n"})," characters are used. Returns a new ",(0,i.jsx)(n.code,{children:"HybridBigInt"})," representing the n-gram."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const trigram = try encoder.encodeNGram("cat");\n// trigram now holds a HybridBigInt representing the "cat" trigram\n'})}),"\n",(0,i.jsx)(n.h4,{id:"encodeallngramsself-ngramencoder-allocator-allocator-str-const-u8-hybridbigint",children:(0,i.jsx)(n.code,{children:"encodeAllNGrams(self: *NGramEncoder, allocator: Allocator, str: []const u8) ![]HybridBigInt"})}),"\n",(0,i.jsxs)(n.p,{children:["Extracts and encodes all overlapping n-grams from a string. For a string of length ",(0,i.jsx)(n.code,{children:"L"})," and n-gram size ",(0,i.jsx)(n.code,{children:"n"}),", this produces ",(0,i.jsx)(n.code,{children:"L - n + 1"})," vectors. If the string is shorter than ",(0,i.jsx)(n.code,{children:"n"}),", a single partial n-gram is returned. The caller owns the returned slice."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const ngrams = try encoder.encodeAllNGrams(allocator, "hello");\n// For trigrams: produces ["hel", "ell", "llo"] -> 3 vectors\n'})}),"\n",(0,i.jsx)(n.h2,{id:"sequencememory",children:"SequenceMemory"}),"\n",(0,i.jsxs)(n.p,{children:["Stores labeled text sequences as bundled n-gram vectors and supports associative retrieval via cosine similarity. Internally manages its own ",(0,i.jsx)(n.code,{children:"ItemMemory"})," and ",(0,i.jsx)(n.code,{children:"NGramEncoder"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"encoding-pipeline",children:"Encoding Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"text -> [n-grams] -> [encode each] -> bundle all -> single vector\n"})}),"\n",(0,i.jsx)(n.p,{children:"The sequence vector is the element-wise majority vote (bundle) of all n-gram vectors."}),"\n",(0,i.jsx)(n.h3,{id:"construction-2",children:"Construction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const SequenceMemory = @import("sequence_hdc").SequenceMemory;\n\nvar memory = SequenceMemory.init(allocator, 1000, 3, 42);\ndefer memory.deinit();\n\n// Optional: enable JIT for faster operations\nmemory.enableJIT();\n'})}),"\n",(0,i.jsx)(n.h3,{id:"methods-2",children:"Methods"}),"\n",(0,i.jsx)(n.h4,{id:"initallocator-allocator-dimension-usize-n-usize-seed-u64-sequencememory",children:(0,i.jsx)(n.code,{children:"init(allocator: Allocator, dimension: usize, n: usize, seed: u64) SequenceMemory"})}),"\n",(0,i.jsx)(n.p,{children:"Creates a new sequence memory. Parameters:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dimension"})," -- hypervector dimension (higher means more capacity, typically 1000--10000)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"n"})," -- n-gram size (typically 3 for text)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"seed"})," -- random seed for item memory"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"deinitself-sequencememory-void",children:(0,i.jsx)(n.code,{children:"deinit(self: *SequenceMemory) void"})}),"\n",(0,i.jsx)(n.p,{children:"Frees all stored sequences, the item memory, and the JIT engine (if enabled)."}),"\n",(0,i.jsx)(n.h4,{id:"enablejitself-sequencememory-void",children:(0,i.jsx)(n.code,{children:"enableJIT(self: *SequenceMemory) void"})}),"\n",(0,i.jsx)(n.p,{children:"Enables JIT acceleration for all internal operations (bind, bundle, cosine similarity). Must be called after the struct is in its final memory location (not before a move)."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["Call ",(0,i.jsx)(n.code,{children:"enableJIT()"})," only after the ",(0,i.jsx)(n.code,{children:"SequenceMemory"})," is at its final memory address. Moving the struct after enabling JIT causes undefined behavior."]})}),"\n",(0,i.jsx)(n.h4,{id:"encodeself-sequencememory-str-const-u8-hybridbigint",children:(0,i.jsx)(n.code,{children:"encode(self: *SequenceMemory, str: []const u8) !HybridBigInt"})}),"\n",(0,i.jsxs)(n.p,{children:["Encodes a string into a single hypervector by extracting all n-grams, encoding each, and bundling them via majority vote. Returns a new ",(0,i.jsx)(n.code,{children:"HybridBigInt"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const vec = try memory.encode("hello world");\n// vec is a single hypervector representing the entire string\n'})}),"\n",(0,i.jsx)(n.h4,{id:"storeself-sequencememory-label-const-u8-str-const-u8-void",children:(0,i.jsx)(n.code,{children:"store(self: *SequenceMemory, label: []const u8, str: []const u8) !void"})}),"\n",(0,i.jsx)(n.p,{children:"Encodes the string and stores the resulting vector with the given label. The label is duplicated internally."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'try memory.store("greeting", "hello world");\ntry memory.store("farewell", "goodbye world");\n'})}),"\n",(0,i.jsx)(n.h4,{id:"queryself-sequencememory-str-const-u8-queryresult",children:(0,i.jsx)(n.code,{children:"query(self: *SequenceMemory, str: []const u8) !?QueryResult"})}),"\n",(0,i.jsxs)(n.p,{children:["Encodes the query string, then compares it against all stored sequences using cosine similarity. Returns the most similar match, or ",(0,i.jsx)(n.code,{children:"null"})," if no sequences are stored."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'if (try memory.query("hi there world")) |result| {\n    // result.label = "greeting"\n    // result.similarity = 0.28 (approximate)\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"querytopkself-sequencememory-str-const-u8-k-usize-queryresult",children:(0,i.jsx)(n.code,{children:"queryTopK(self: *SequenceMemory, str: []const u8, k: usize) ![]QueryResult"})}),"\n",(0,i.jsx)(n.p,{children:"Returns the top-k most similar stored sequences, sorted by descending similarity. The caller owns the returned slice."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const results = try memory.queryTopK("hi there world", 3);\n// results[0] is the best match, results[1] second-best, etc.\n'})}),"\n",(0,i.jsx)(n.h3,{id:"queryresult",children:"QueryResult"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:"pub const QueryResult = struct {\n    label: []const u8,\n    similarity: f64,\n};\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"label"})," field points to the internally stored label string. The ",(0,i.jsx)(n.code,{children:"similarity"})," field is the cosine similarity in the range ",(0,i.jsx)(n.code,{children:"[-1, 1]"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"languagedetector",children:"LanguageDetector"}),"\n",(0,i.jsxs)(n.p,{children:["A ready-to-use HDC application that identifies the language of input text. It wraps ",(0,i.jsx)(n.code,{children:"SequenceMemory"})," with a simplified train/detect interface."]}),"\n",(0,i.jsx)(n.h3,{id:"construction-3",children:"Construction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const LanguageDetector = @import("sequence_hdc").LanguageDetector;\n\nvar detector = LanguageDetector.init(allocator, 4000, 3, 42);\ndefer detector.deinit();\n\n// Enable JIT after init (when struct is at its final address)\ndetector.enableJIT();\n'})}),"\n",(0,i.jsx)(n.h3,{id:"methods-3",children:"Methods"}),"\n",(0,i.jsx)(n.h4,{id:"initallocator-allocator-dimension-usize-n-usize-seed-u64-languagedetector",children:(0,i.jsx)(n.code,{children:"init(allocator: Allocator, dimension: usize, n: usize, seed: u64) LanguageDetector"})}),"\n",(0,i.jsx)(n.p,{children:"Creates a new language detector. Use a dimension of at least 4000 for reliable multi-language detection."}),"\n",(0,i.jsx)(n.h4,{id:"enablejitself-languagedetector-void",children:(0,i.jsx)(n.code,{children:"enableJIT(self: *LanguageDetector) void"})}),"\n",(0,i.jsx)(n.p,{children:"Enables JIT acceleration for all operations."}),"\n",(0,i.jsx)(n.h4,{id:"deinitself-languagedetector-void",children:(0,i.jsx)(n.code,{children:"deinit(self: *LanguageDetector) void"})}),"\n",(0,i.jsx)(n.p,{children:"Frees all resources."}),"\n",(0,i.jsx)(n.h4,{id:"trainself-languagedetector-language-const-u8-sample-const-u8-void",children:(0,i.jsx)(n.code,{children:"train(self: *LanguageDetector, language: []const u8, sample: []const u8) !void"})}),"\n",(0,i.jsxs)(n.p,{children:['Trains on a labeled text sample. The language label (e.g., "english") is stored with the encoded sample. Call ',(0,i.jsx)(n.code,{children:"train"})," multiple times with different samples to improve accuracy for a given language."]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"Longer training samples produce better results. Aim for at least 50 characters per language sample. Multiple samples per language are even better."})}),"\n",(0,i.jsx)(n.h4,{id:"detectself-languagedetector-text-const-u8-queryresult",children:(0,i.jsx)(n.code,{children:"detect(self: *LanguageDetector, text: []const u8) !?QueryResult"})}),"\n",(0,i.jsxs)(n.p,{children:["Detects the language of the input text by finding the most similar training sample. Returns a ",(0,i.jsx)(n.code,{children:"QueryResult"})," with the language label and similarity score, or ",(0,i.jsx)(n.code,{children:"null"})," if no training data exists."]}),"\n",(0,i.jsx)(n.h2,{id:"complete-example-language-detection",children:"Complete Example: Language Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-zig",children:'const std = @import("std");\nconst sequence_hdc = @import("sequence_hdc");\n\npub fn main() !void {\n    const allocator = std.heap.page_allocator;\n\n    // Create detector with 4000-dimensional vectors, trigrams, seed=42\n    var detector = sequence_hdc.LanguageDetector.init(allocator, 4000, 3, 42);\n    defer detector.deinit();\n\n    // Enable JIT for faster operations\n    detector.enableJIT();\n\n    // Train with language samples\n    try detector.train("english",\n        "the quick brown fox jumps over the lazy dog and runs through the forest");\n    try detector.train("german",\n        "der schnelle braune fuchs springt ueber den faulen hund und rennt durch den wald");\n    try detector.train("spanish",\n        "el rapido zorro marron salta sobre el perro perezoso y corre por el bosque");\n\n    // Detect language of new text\n    const samples = [_][]const u8{\n        "the cat and the dog are running through the park",\n        "der hund und die katze rennen durch den garten",\n        "el gato y el perro corren por el parque",\n    };\n\n    for (samples) |text| {\n        if (try detector.detect(text)) |result| {\n            std.debug.print("Text: \\"{s}\\"\\n", .{text});\n            std.debug.print("  Language: {s} (similarity: {d:.4})\\n\\n", .{\n                result.label,\n                result.similarity,\n            });\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Text: "the cat and the dog are running through the park"\n  Language: english (similarity: 0.3842)\n\nText: "der hund und die katze rennen durch den garten"\n  Language: german (similarity: 0.4156)\n\nText: "el gato y el perro corren por el parque"\n  Language: spanish (similarity: 0.3921)\n'})}),"\n",(0,i.jsxs)(n.p,{children:["All types support optional JIT acceleration via ",(0,i.jsx)(n.code,{children:"JitVSAEngine"})," (see ",(0,i.jsx)(n.a,{href:"/trinity/docs/api/jit",children:"JIT API"}),")."]}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsx)("summary",{children:"Why Ternary HDC?"}),(0,i.jsx)(n.p,{children:"Ternary vectors ({-1, 0, +1}) offer concrete advantages over binary or real-valued HDC:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1.58 bits/trit"})," information density (vs 1 bit for binary)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"No multiplication"})," needed for bind (ternary multiply is just sign logic)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sparse-friendly:"})," zero trits require no computation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"20x memory savings"})," vs float32 representations."]}),"\n"]}),(0,i.jsx)(n.p,{children:"These properties make ternary HDC practical for edge devices and embedded systems where memory and compute are constrained."})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>c});var t=r(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);