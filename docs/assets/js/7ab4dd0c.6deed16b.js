"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[2602],{5114(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>a,frontMatter:()=>d,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"deployment/runpod","title":"RunPod GPU Deployment","description":"RunPod provides on-demand GPU instances suitable for high-throughput BitNet b1.58 inference. This guide walks through deploying Trinity on a RunPod instance for benchmarking and production inference.","source":"@site/docs/deployment/runpod.md","sourceDirName":"deployment","slug":"/deployment/runpod","permalink":"/trinity/docs/deployment/runpod","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/deployment/runpod.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Deployment","permalink":"/trinity/docs/deployment/"},"next":{"title":"Local Deployment","permalink":"/trinity/docs/deployment/local"}}');var s=t(4848),r=t(8453);const d={sidebar_position:2},o="RunPod GPU Deployment",l={},c=[{value:"Recommended GPU Types",id:"recommended-gpu-types",level:2},{value:"Setup Steps",id:"setup-steps",level:2},{value:"1. Create a RunPod Instance",id:"1-create-a-runpod-instance",level:3},{value:"2. Connect and Run the Benchmark Script",id:"2-connect-and-run-the-benchmark-script",level:3},{value:"3. Manual Setup (Alternative)",id:"3-manual-setup-alternative",level:3},{value:"4. Server Mode for API Access",id:"4-server-mode-for-api-access",level:3},{value:"Cost Considerations",id:"cost-considerations",level:2},{value:"Output Files",id:"output-files",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"runpod-gpu-deployment",children:"RunPod GPU Deployment"})}),"\n",(0,s.jsx)(n.p,{children:"RunPod provides on-demand GPU instances suitable for high-throughput BitNet b1.58 inference. This guide walks through deploying Trinity on a RunPod instance for benchmarking and production inference."}),"\n",(0,s.jsx)(n.h2,{id:"recommended-gpu-types",children:"Recommended GPU Types"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"GPU"}),(0,s.jsx)(n.th,{children:"VRAM"}),(0,s.jsx)(n.th,{children:"Cost Tier"}),(0,s.jsx)(n.th,{children:"Expected Throughput (2B model)"}),(0,s.jsx)(n.th,{children:"Best For"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"NVIDIA H100 SXM"}),(0,s.jsx)(n.td,{children:"80 GB"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"~300K+ tok/s"}),(0,s.jsx)(n.td,{children:"Maximum performance, AVX-512 VNNI on CPU"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"NVIDIA A100 80GB"}),(0,s.jsx)(n.td,{children:"80 GB"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"~274K tok/s"}),(0,s.jsx)(n.td,{children:"Production workloads"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"NVIDIA RTX 3090"}),(0,s.jsx)(n.td,{children:"24 GB"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"~298K tok/s"}),(0,s.jsx)(n.td,{children:"Cost-effective benchmarking"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"NVIDIA RTX 4090"}),(0,s.jsx)(n.td,{children:"24 GB"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"~310K tok/s"}),(0,s.jsx)(n.td,{children:"Consumer-grade best performance"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"For the BitNet b1.58-2B-4T model (1.1 GB GGUF), even a 24 GB GPU has more than sufficient VRAM. The CPU capabilities of the host (particularly AVX-512 VNNI support) also significantly affect throughput, as bitnet.cpp uses CPU kernels for ternary weight unpacking."}),"\n",(0,s.jsx)(n.h2,{id:"setup-steps",children:"Setup Steps"}),"\n",(0,s.jsx)(n.h3,{id:"1-create-a-runpod-instance",children:"1. Create a RunPod Instance"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Sign up at ",(0,s.jsx)(n.a,{href:"https://runpod.io",children:"runpod.io"})]}),"\n",(0,s.jsx)(n.li,{children:"Create a new GPU pod with your chosen GPU type"}),"\n",(0,s.jsx)(n.li,{children:"Select a PyTorch or Ubuntu template (provides CUDA and Python)"}),"\n",(0,s.jsx)(n.li,{children:"Ensure at least 20 GB of disk space for the model and build tools"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-connect-and-run-the-benchmark-script",children:"2. Connect and Run the Benchmark Script"}),"\n",(0,s.jsx)(n.p,{children:"Trinity includes a pre-built benchmark script for H100 instances:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# SSH into your RunPod instance, then:\ngit clone https://github.com/gHashTag/trinity.git\ncd trinity\n\n# Run the automated benchmark script\nbash scripts/runpod_h100_bitnet.sh\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The script (",(0,s.jsx)(n.code,{children:"scripts/runpod_h100_bitnet.sh"}),") automates the entire process:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verifies hardware (CPU features, GPU type, AVX-512 support)"}),"\n",(0,s.jsx)(n.li,{children:"Installs dependencies (clang, cmake, Python packages)"}),"\n",(0,s.jsx)(n.li,{children:"Clones and builds Microsoft's bitnet.cpp with optimized kernels"}),"\n",(0,s.jsx)(n.li,{children:"Downloads the BitNet b1.58-2B-4T GGUF model from HuggingFace"}),"\n",(0,s.jsx)(n.li,{children:"Runs thread scaling tests (1, 2, 4, 8, 16, max threads)"}),"\n",(0,s.jsx)(n.li,{children:"Executes 12 diverse prompts with 500-token generation each"}),"\n",(0,s.jsx)(n.li,{children:"Produces a results report and JSON metrics file"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-manual-setup-alternative",children:"3. Manual Setup (Alternative)"}),"\n",(0,s.jsx)(n.p,{children:"If you prefer manual control:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Install Zig\ncurl -LO https://ziglang.org/download/0.13.0/zig-linux-x86_64-0.13.0.tar.xz\ntar -xf zig-linux-x86_64-0.13.0.tar.xz\nexport PATH="$PWD/zig-linux-x86_64-0.13.0:$PATH"\n\n# Clone and build Trinity\ngit clone https://github.com/gHashTag/trinity.git\ncd trinity\nzig build firebird\n\n# For bitnet.cpp inference, follow the benchmark script steps\n'})}),"\n",(0,s.jsx)(n.h3,{id:"4-server-mode-for-api-access",children:"4. Server Mode for API Access"}),"\n",(0,s.jsx)(n.p,{children:"To expose inference as an HTTP API on your RunPod instance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./bin/vibee serve --port 8080\n"})}),"\n",(0,s.jsx)(n.p,{children:"Configure the RunPod instance to expose port 8080 via the RunPod proxy URL, which provides HTTPS access to your running service."}),"\n",(0,s.jsx)(n.h2,{id:"cost-considerations",children:"Cost Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RunPod charges by the hour for GPU instances. Stop your pod when not in use."}),"\n",(0,s.jsx)(n.li,{children:"The H100 SXM is the most expensive but provides the best throughput."}),"\n",(0,s.jsx)(n.li,{children:"For cost-effective testing, the RTX 3090 delivers comparable per-token throughput at a fraction of the cost."}),"\n",(0,s.jsx)(n.li,{children:"A single benchmark run (12 prompts, 500 tokens each) typically completes in under 5 minutes on H100 hardware."}),"\n",(0,s.jsx)(n.li,{children:"The benchmark script reminds you to stop the pod when finished."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"output-files",children:"Output Files"}),"\n",(0,s.jsx)(n.p,{children:"After running the benchmark script, results are saved to:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"File"}),(0,s.jsx)(n.th,{children:"Contents"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/root/bitnet_h100_results.txt"})}),(0,s.jsx)(n.td,{children:"Human-readable results with all prompts and outputs"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/root/bitnet_h100_metrics.json"})}),(0,s.jsx)(n.td,{children:"Machine-readable JSON with per-test timing and throughput"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Missing AVX-512"}),": Some RunPod instances use older CPUs. The script detects this and falls back to a manual cmake build without TL2 optimizations."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build failures"}),": Ensure clang and cmake are installed. The script handles this automatically."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tokenizer warnings"}),': The GGUF model may show a "missing pre-tokenizer type" warning. The benchmark script overrides this with ',(0,s.jsx)(n.code,{children:'--override-kv "tokenizer.ggml.pre=str:llama-bpe"'}),"."]}),"\n"]})]})}function a(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453(e,n,t){t.d(n,{R:()=>d,x:()=>o});var i=t(6540);const s={},r=i.createContext(s);function d(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);