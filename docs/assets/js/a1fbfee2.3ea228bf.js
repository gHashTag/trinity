"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[1867],{5096(e,i,n){n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"hdc/applications","title":"HDC Applications","description":"Trinity includes 23 Hyperdimensional Computing application modules, each defined as a .vibee specification in the specs/tri/ directory. These modules cover a broad range of machine learning and AI tasks, all implemented using the same ternary Vector Symbolic Architecture operations -- no gradient descent, no backpropagation, and no floating-point weight matrices.","source":"@site/docs/hdc/applications.md","sourceDirName":"hdc","slug":"/hdc/applications","permalink":"/trinity/docs/hdc/applications","draft":false,"unlisted":false,"editUrl":"https://github.com/gHashTag/trinity/tree/main/docsite/docs/hdc/applications.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Hyperdimensional Computing","permalink":"/trinity/docs/hdc/"},"next":{"title":"IGLA GloVe Competitor Comparison","permalink":"/trinity/docs/hdc/igla-glove-comparison"}}');var r=n(4848),t=n(8453);const s={sidebar_position:2},o="HDC Applications",d={},c=[{value:"Classification",id:"classification",level:2},{value:"HDC Classifier (<code>hdc_classifier.vibee</code>)",id:"hdc-classifier-hdc_classifiervibee",level:3},{value:"HDC Stream Classifier (<code>hdc_stream_classifier.vibee</code>)",id:"hdc-stream-classifier-hdc_stream_classifiervibee",level:3},{value:"HDC Few-Shot Learner (<code>hdc_few_shot.vibee</code>)",id:"hdc-few-shot-learner-hdc_few_shotvibee",level:3},{value:"HDC Multi-Task Learner (<code>hdc_multi_task.vibee</code>)",id:"hdc-multi-task-learner-hdc_multi_taskvibee",level:3},{value:"Clustering and Unsupervised Learning",id:"clustering-and-unsupervised-learning",level:2},{value:"HDC Clustering (<code>hdc_clustering.vibee</code>)",id:"hdc-clustering-hdc_clusteringvibee",level:3},{value:"Natural Language Processing",id:"natural-language-processing",level:2},{value:"HDC Text Encoder (<code>hdc_text_encoder.vibee</code>)",id:"hdc-text-encoder-hdc_text_encodervibee",level:3},{value:"HDC Language Model (<code>hdc_language_model.vibee</code>)",id:"hdc-language-model-hdc_language_modelvibee",level:3},{value:"HDC Sequence Predictor (<code>hdc_sequence_predictor.vibee</code>)",id:"hdc-sequence-predictor-hdc_sequence_predictorvibee",level:3},{value:"Knowledge Representation and Reasoning",id:"knowledge-representation-and-reasoning",level:2},{value:"HDC Knowledge Graph (<code>hdc_knowledge_graph.vibee</code>)",id:"hdc-knowledge-graph-hdc_knowledge_graphvibee",level:3},{value:"HDC Graph Traversal (<code>hdc_graph_traversal.vibee</code>)",id:"hdc-graph-traversal-hdc_graph_traversalvibee",level:3},{value:"HDC Symbolic Reasoning (<code>hdc_symbolic_reasoning.vibee</code>)",id:"hdc-symbolic-reasoning-hdc_symbolic_reasoningvibee",level:3},{value:"HDC Associative Memory (<code>hdc_associative_memory.vibee</code>)",id:"hdc-associative-memory-hdc_associative_memoryvibee",level:3},{value:"Anomaly Detection",id:"anomaly-detection",level:2},{value:"HDC Anomaly Detector (<code>hdc_anomaly_detector.vibee</code>)",id:"hdc-anomaly-detector-hdc_anomaly_detectorvibee",level:3},{value:"HDC Temporal Anomaly Detector (<code>hdc_temporal_anomaly.vibee</code>)",id:"hdc-temporal-anomaly-detector-hdc_temporal_anomalyvibee",level:3},{value:"Recommendation",id:"recommendation",level:2},{value:"HDC Recommender (<code>hdc_recommender.vibee</code>)",id:"hdc-recommender-hdc_recommendervibee",level:3},{value:"Search and Retrieval",id:"search-and-retrieval",level:2},{value:"HDC Semantic Search (<code>hdc_semantic_search.vibee</code>)",id:"hdc-semantic-search-hdc_semantic_searchvibee",level:3},{value:"Ensemble and Cognitive Systems",id:"ensemble-and-cognitive-systems",level:2},{value:"HDC Ensemble (<code>hdc_ensemble.vibee</code>)",id:"hdc-ensemble-hdc_ensemblevibee",level:3},{value:"HDC Cognitive Agent (<code>hdc_cognitive.vibee</code>)",id:"hdc-cognitive-agent-hdc_cognitivevibee",level:3},{value:"Explainability",id:"explainability",level:2},{value:"HDC Explainer (<code>hdc_explainer.vibee</code>)",id:"hdc-explainer-hdc_explainervibee",level:3},{value:"Learning Paradigms",id:"learning-paradigms",level:2},{value:"HDC Continual Learning (<code>hdc_continual_learning.vibee</code>)",id:"hdc-continual-learning-hdc_continual_learningvibee",level:3},{value:"HDC Federated Learning (<code>hdc_federated.vibee</code>)",id:"hdc-federated-learning-hdc_federatedvibee",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:2},{value:"HDC RL Agent (<code>hdc_rl_agent.vibee</code>)",id:"hdc-rl-agent-hdc_rl_agentvibee",level:3},{value:"Multimodal Processing",id:"multimodal-processing",level:2},{value:"HDC Multimodal Classifier (<code>hdc_multimodal.vibee</code>)",id:"hdc-multimodal-classifier-hdc_multimodalvibee",level:3},{value:"Source Specifications",id:"source-specifications",level:2}];function l(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"hdc-applications",children:"HDC Applications"})}),"\n",(0,r.jsxs)(i.p,{children:["Trinity includes 23 Hyperdimensional Computing application modules, each defined as a ",(0,r.jsx)(i.code,{children:".vibee"})," specification in the ",(0,r.jsx)(i.code,{children:"specs/tri/"})," directory. These modules cover a broad range of machine learning and AI tasks, all implemented using the same ternary Vector Symbolic Architecture operations -- no gradient descent, no backpropagation, and no floating-point weight matrices."]}),"\n",(0,r.jsxs)(i.p,{children:["All modules share a common encoding foundation through ",(0,r.jsx)(i.code,{children:"HDCTextEncoder"})," and ",(0,r.jsx)(i.code,{children:"ItemMemory"}),", ensuring consistent vector representations across the entire application suite."]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"classification",children:"Classification"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-classifier-hdc_classifiervibee",children:["HDC Classifier (",(0,r.jsx)(i.code,{children:"hdc_classifier.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"One-shot and few-shot text classification using bundled class prototypes. Training encodes text samples as hypervectors and bundles them into per-class prototypes via majority vote. Prediction finds the class prototype with the highest cosine similarity to the encoded query. Supports incremental addition of new classes without retraining."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-stream-classifier-hdc_stream_classifiervibee",children:["HDC Stream Classifier (",(0,r.jsx)(i.code,{children:"hdc_stream_classifier.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Adaptive online classifier for data streams with concept drift detection. Maintains a sliding window of recent labeled samples and periodically rebuilds class prototypes from the window contents. Monitors prediction confidence over time to detect distribution shifts, alerting when the drift score exceeds a configurable threshold."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-few-shot-learner-hdc_few_shotvibee",children:["HDC Few-Shot Learner (",(0,r.jsx)(i.code,{children:"hdc_few_shot.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Meta-learning for K-shot classification with prototype rectification. Learns new classes from as few as 1-5 examples per class. Applies centroid subtraction to remove shared background components from prototypes, amplifying class-specific signals and improving separation between classes with limited training data."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-multi-task-learner-hdc_multi_taskvibee",children:["HDC Multi-Task Learner (",(0,r.jsx)(i.code,{children:"hdc_multi_task.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Simultaneous multi-label classification with independent task heads sharing a single encoder. Text is encoded once, then compared against separate prototype banks for each task (e.g., sentiment, topic, language). Tasks cannot interfere with each other because prototype banks are fully independent."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"clustering-and-unsupervised-learning",children:"Clustering and Unsupervised Learning"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-clustering-hdc_clusteringvibee",children:["HDC Clustering (",(0,r.jsx)(i.code,{children:"hdc_clustering.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Unsupervised K-means clustering in hyperdimensional space. Cluster centroids are computed via bundling (majority vote over assigned samples), and assignment uses cosine similarity. Supports random and k-means++ initialization strategies, with convergence tracking via centroid drift measurement and silhouette scoring for cluster quality evaluation."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"natural-language-processing",children:"Natural Language Processing"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-text-encoder-hdc_text_encodervibee",children:["HDC Text Encoder (",(0,r.jsx)(i.code,{children:"hdc_text_encoder.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Shared text encoding module used by all downstream applications. Supports four encoding modes: character n-gram bundling, word-level encoding with positional permutation, word-level encoding with TF-IDF weighting, and a hybrid mode combining character and word representations."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-language-model-hdc_language_modelvibee",children:["HDC Language Model (",(0,r.jsx)(i.code,{children:"hdc_language_model.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Character-level language model for next-character prediction and text generation. Stores context vectors bundled per next character, and predicts by finding the most similar stored context. Supports temperature-controlled softmax sampling, top-k filtering, repetition penalty, and perplexity measurement."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-sequence-predictor-hdc_sequence_predictorvibee",children:["HDC Sequence Predictor (",(0,r.jsx)(i.code,{children:"hdc_sequence_predictor.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Word-level next-token prediction using sliding n-gram windows with positional encoding. Trains by extracting context windows and storing them alongside target words. Supports greedy prediction, top-k candidate ranking, iterative text generation, and multi-step beam search for improved sequence quality."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"knowledge-representation-and-reasoning",children:"Knowledge Representation and Reasoning"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-knowledge-graph-hdc_knowledge_graphvibee",children:["HDC Knowledge Graph (",(0,r.jsx)(i.code,{children:"hdc_knowledge_graph.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Stores subject-relation-object triples in holographic distributed memory. Triples are encoded as three-way bindings and bundled into a single memory vector. Supports pattern-matching queries with wildcards: given two of three elements, the system recovers the missing element by unbinding from memory and decoding against the entity codebook."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-graph-traversal-hdc_graph_traversalvibee",children:["HDC Graph Traversal (",(0,r.jsx)(i.code,{children:"hdc_graph_traversal.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:'Multi-hop reasoning and path queries over HDC knowledge graphs. Chains single-hop queries to traverse paths through the graph. Supports analogy queries ("a is to b as c is to ?") via vector arithmetic, and subgraph matching to find entities satisfying multiple relation constraints simultaneously.'}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-symbolic-reasoning-hdc_symbolic_reasoningvibee",children:["HDC Symbolic Reasoning (",(0,r.jsx)(i.code,{children:"hdc_symbolic_reasoning.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Logic and analogy engine using role-filler bindings in hyperdimensional space. Represents structured concepts as frames composed of bound role-filler pairs. Supports frame queries (slot access via unbinding), analogy solving (extracting and applying relations between concepts), and compositional reasoning over the vocabulary."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-associative-memory-hdc_associative_memoryvibee",children:["HDC Associative Memory (",(0,r.jsx)(i.code,{children:"hdc_associative_memory.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Content-addressable key-value store using holographic distributed memory. Key-value pairs are bound together and bundled into a single memory vector. Queries unbind the key from memory and decode the result against a value codebook. Supports approximate queries with noisy or partial keys and periodic cleanup to reduce accumulated noise."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"anomaly-detection",children:"Anomaly Detection"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-anomaly-detector-hdc_anomaly_detectorvibee",children:["HDC Anomaly Detector (",(0,r.jsx)(i.code,{children:"hdc_anomaly_detector.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:'One-class anomaly detection that learns a "normal" prototype from positive examples only. Anomaly scores are computed as the distance from the normal prototype (1 minus cosine similarity). Threshold is automatically calibrated from training data statistics using a configurable sensitivity parameter. Supports multiple independent normal profiles.'}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-temporal-anomaly-detector-hdc_temporal_anomalyvibee",children:["HDC Temporal Anomaly Detector (",(0,r.jsx)(i.code,{children:"hdc_temporal_anomaly.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Time-series anomaly detection over event sequences using sliding context windows. Encodes windows with positional permutation to preserve temporal order, then scores against learned normal profiles. Includes exponential moving average smoothing for noisy streams and multi-regime support for different normal patterns (e.g., daytime vs. nighttime)."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"recommendation",children:"Recommendation"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-recommender-hdc_recommendervibee",children:["HDC Recommender (",(0,r.jsx)(i.code,{children:"hdc_recommender.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Content-based and collaborative filtering recommendation system. User profiles are constructed by bundling the hypervectors of liked items, and recommendations are generated by ranking unseen items by cosine similarity to the user profile. Supports collaborative filtering by finding users with similar profiles and recommending from the community signal. Handles cold-start naturally -- a single rating produces a functional profile."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"search-and-retrieval",children:"Search and Retrieval"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-semantic-search-hdc_semantic_searchvibee",children:["HDC Semantic Search (",(0,r.jsx)(i.code,{children:"hdc_semantic_search.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Document retrieval via hyperdimensional similarity. Documents are encoded as hypervectors and stored in an index. Queries are encoded with the same method, and the top-k most similar documents are returned by cosine ranking. Supports TF-IDF weighting for relevance-based ranking and incremental indexing without full rebuild."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"ensemble-and-cognitive-systems",children:"Ensemble and Cognitive Systems"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-ensemble-hdc_ensemblevibee",children:["HDC Ensemble (",(0,r.jsx)(i.code,{children:"hdc_ensemble.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Unified cognitive pipeline combining a supervised classifier, an anomaly detector, and unsupervised clustering into a single prediction system. Anomaly gating rejects out-of-distribution inputs before classification. Confidence thresholding refuses low-confidence predictions. All subsystems share identical encoding for consistency."}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-cognitive-agent-hdc_cognitivevibee",children:["HDC Cognitive Agent (",(0,r.jsx)(i.code,{children:"hdc_cognitive.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Full cognitive loop integrating perception, classification, episodic memory, anomaly detection, explainability, and incremental learning into a single processing pipeline. Each input is encoded once and passed through all subsystems: classify, recall similar past inputs, detect novelty, explain the decision via word attribution, and learn from the experience. Supports supervised, self-supervised, and memory-only learning modes."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"explainability",children:"Explainability"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-explainer-hdc_explainervibee",children:["HDC Explainer (",(0,r.jsx)(i.code,{children:"hdc_explainer.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:'Feature attribution for classifier decisions using VSA algebra. Explains predictions by computing per-word cosine similarity to class prototypes, revealing which words contributed most to a classification. Supports contrastive explanations ("why class A instead of class B?") by computing attribution differences between two class prototypes.'}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"learning-paradigms",children:"Learning Paradigms"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-continual-learning-hdc_continual_learningvibee",children:["HDC Continual Learning (",(0,r.jsx)(i.code,{children:"hdc_continual_learning.vibee"}),")"]}),"\n",(0,r.jsxs)(i.p,{children:["Incremental class learning with near-zero catastrophic forgetting. New classes are added by creating new prototypes without modifying existing ones. Tested across 10 phases with 20 classes: average forgetting 3.04%, maximum forgetting 12.5% (compared to 50-90% catastrophic forgetting in neural networks). No replay buffer or regularization is needed because prototypes are fully independent. See ",(0,r.jsx)(i.a,{href:"/docs/hdc/",children:"HDC Overview"})," for full benchmark results."]}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-federated-learning-hdc_federatedvibee",children:["HDC Federated Learning (",(0,r.jsx)(i.code,{children:"hdc_federated.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Privacy-preserving distributed classification where multiple nodes train locally on private data and share only prototype hypervectors with a central coordinator. The global model is constructed by bundling prototypes from all nodes. Raw data never leaves the local node, and prototypes are high-dimensional bundled averages from which individual samples cannot be recovered."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-rl-agent-hdc_rl_agentvibee",children:["HDC RL Agent (",(0,r.jsx)(i.code,{children:"hdc_rl_agent.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Q-learning via hyperdimensional prototypes for reinforcement learning in discrete environments. Maintains positive and negative prototype vectors for each action, bundling state vectors according to their discounted returns. Q-values are estimated as the cosine difference between positive and negative prototypes for a given state-action pair. Includes a gridworld environment for training and evaluation with epsilon-greedy exploration."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"multimodal-processing",children:"Multimodal Processing"}),"\n",(0,r.jsxs)(i.h3,{id:"hdc-multimodal-classifier-hdc_multimodalvibee",children:["HDC Multimodal Classifier (",(0,r.jsx)(i.code,{children:"hdc_multimodal.vibee"}),")"]}),"\n",(0,r.jsx)(i.p,{children:"Fusion of text, numeric, categorical, and boolean features into a unified hyperdimensional representation. Numeric values are encoded using thermometer coding (close values share most level vectors, producing high similarity). Features are bound with role vectors and bundled with text encodings to produce a single fused hypervector for classification."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"source-specifications",children:"Source Specifications"}),"\n",(0,r.jsxs)(i.p,{children:["All 23 HDC application modules are defined as ",(0,r.jsx)(i.code,{children:".vibee"})," specifications in the ",(0,r.jsx)(i.code,{children:"specs/tri/"})," directory of the Trinity repository. Each specification file documents the module's types, behaviors, and architectural design. Code is generated from these specifications using the VIBEE compiler:"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"./bin/vibee gen specs/tri/hdc_classifier.vibee\n"})}),"\n",(0,r.jsxs)(i.p,{children:["Refer to the individual ",(0,r.jsx)(i.code,{children:".vibee"})," files for complete type definitions, behavior contracts, and detailed architectural descriptions."]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453(e,i,n){n.d(i,{R:()=>s,x:()=>o});var a=n(6540);const r={},t=a.createContext(r);function s(e){const i=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(t.Provider,{value:i},e.children)}}}]);